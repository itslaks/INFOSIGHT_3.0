from flask import Flask, Blueprint, render_template, request, jsonify, g
from flask_cors import CORS
from utils.security import rate_limit_strict, InputValidator
from PIL import Image, ImageStat, ImageFilter, ImageEnhance, ImageChops, ImageDraw
import torch
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.models as models
import io
import numpy as np
from datetime import datetime
import hashlib
import cv2
import os
import threading
from functools import lru_cache
import logging
import re
from collections import Counter

# Configure logging first (before any logger usage)
logger = logging.getLogger(__name__)

# Advanced libraries for v4.0
try:
    from scipy import stats, ndimage, signal
    from scipy.fft import dctn
    from scipy.ndimage import gaussian_filter
    try:
        from skimage import feature, measure, filters, morphology
        from skimage.restoration import denoise_tv_chambolle
        SKIMAGE_AVAILABLE = True
    except ImportError:
        SKIMAGE_AVAILABLE = False
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    SKIMAGE_AVAILABLE = False
    logger.warning("‚ö†Ô∏è scipy/scikit-image not available - advanced features will be limited")

# Perceptual hashing
try:
    import imagehash
    IMAGEHASH_AVAILABLE = True
except ImportError:
    IMAGEHASH_AVAILABLE = False
    logger.warning("‚ö†Ô∏è imagehash not available - perceptual hashing disabled")

# OCR for watermark detection
try:
    import pytesseract
    OCR_AVAILABLE = True
except ImportError:
    OCR_AVAILABLE = False

# Vision Analyzer Integration (Hugging Face + Local Fallback)
try:
    import sys
    from pathlib import Path
    sys.path.insert(0, str(Path(__file__).parent.parent))
    from utils.vision_analyzer import analyze_image, cloud_caption, local_caption, detect_objects, extract_colors
    VISION_ANALYZER_AVAILABLE = True
    logger.info("‚úì Vision Analyzer: AVAILABLE for TrueShot AI")
except ImportError as e:
    VISION_ANALYZER_AVAILABLE = False
    logger.warning(f"‚ö†Ô∏è Vision Analyzer: NOT AVAILABLE ({e})")
    # Create dummy functions
    def analyze_image(*args, **kwargs):
        return {"caption": "", "dominant_colors": [], "objects_detected": [], "analysis_source": "none"}
    def cloud_caption(*args, **kwargs): return None
    def local_caption(*args, **kwargs): return None
    def detect_objects(*args, **kwargs): return []
    def extract_colors(*args, **kwargs): return []

trueshot_ai = Blueprint('trueshot_ai', __name__, template_folder='templates')

# Log application initialization
logger.info("=" * 80)
logger.info("üöÄ TrueShot AI v4.0 - Ultra-Advanced Detection System Initializing")
logger.info("=" * 80)
if not OCR_AVAILABLE:
    logger.warning("‚ö†Ô∏è pytesseract not available - watermark text detection disabled (install: pip install pytesseract)")
if not SCIPY_AVAILABLE:
    logger.warning("‚ö†Ô∏è scipy not available - forensic analysis features will be limited")
if not SKIMAGE_AVAILABLE:
    logger.warning("‚ö†Ô∏è scikit-image not available - advanced texture analysis will be limited")
if not IMAGEHASH_AVAILABLE:
    logger.warning("‚ö†Ô∏è imagehash not available - perceptual hashing disabled")

# Singleton pattern for model instance
_model_instance = None
_model_lock = threading.Lock()

class UltraAdvancedImageAnalyzer:
    """
    TrueShot AI v4.0 - Ultra-Advanced Image Detection System
    Revolutionary features:
    - Forensic analysis (ELA, double JPEG, copy-move, splicing)
    - AI signature detection (GAN fingerprints, diffusion signatures)
    - Advanced texture analysis (LBP, Gabor filters)
    - Face manipulation detection (deepfake indicators)
    - Perceptual hashing & reverse image search
    - Multi-model ensemble with 50+ detection factors
    """
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = self._setup_model()
        self.classes = ['AI-generated', 'Real']
        self.preprocess = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
        
        # Enhanced detection patterns
        self.ai_patterns = {
            'diffusion_artifacts': 0.0,
            'gan_signatures': 0.0,
            'unnatural_smoothness': 0.0,
            'pixel_repetition': 0.0,
            'color_banding': 0.0
        }
        
        # AI service logos and watermarks to detect (2026 standards) - Expanded
        self.ai_service_indicators = {
            'gemini': ['gemini', 'google gemini', 'gemini ai', 'made with gemini', 'gemini.google', 'google ai'],
            'midjourney': ['midjourney', 'mj', 'made with midjourney', 'midjourney ai', 'midjourney.com'],
            'dalle': ['dall-e', 'dalle', 'dall¬∑e', 'openai', 'made with dall-e', 'dall e', 'dall-e 2', 'dall-e 3'],
            'stable_diffusion': ['stable diffusion', 'sd', 'stablediffusion', 'made with stable diffusion', 'sd xl', 'sd 1.5'],
            'leonardo': ['leonardo', 'leonardo.ai', 'made with leonardo', 'leonardo ai'],
            'firefly': ['adobe firefly', 'firefly', 'made with firefly', 'adobe ai'],
            'playground': ['playground ai', 'playground', 'made with playground', 'playgroundai'],
            'craiyon': ['craiyon', 'dalle mini', 'made with craiyon'],
            'nightcafe': ['nightcafe', 'night cafe', 'made with nightcafe', 'nightcafe ai'],
            'artbreeder': ['artbreeder', 'art breeder', 'made with artbreeder'],
            'runway': ['runway', 'runwayml', 'made with runway', 'runway ai', 'runway gen-2'],
            'imagen': ['imagen', 'google imagen', 'made with imagen', 'imagen 2'],
            'flux': ['flux', 'black forest labs', 'flux.1', 'flux dev', 'made with flux'],
            'ideogram': ['ideogram', 'ideogram.ai', 'made with ideogram'],
            'comfyui': ['comfyui', 'comfy ui', 'made with comfyui'],
            'automatic1111': ['automatic1111', 'a1111', 'stable diffusion webui'],
            'civitai': ['civitai', 'civitai.com'],
            'huggingface': ['hugging face', 'huggingface', 'hf space'],
            'replicate': ['replicate', 'replicate.com'],
            'together': ['together ai', 'together.xyz']
        }

    def _setup_model(self):
        """Setup model with improved error handling and singleton pattern"""
        global _model_instance
        
        if _model_instance is not None:
            return _model_instance
        
        with _model_lock:
            if _model_instance is not None:
                return _model_instance
            
            model = models.resnet18(weights=None)
            num_ftrs = model.fc.in_features
            model.fc = nn.Sequential(
                nn.Dropout(p=0.5),
                nn.Linear(num_ftrs, 2)
            )
            
            # Try multiple possible model file locations (relative to project root)
            try:
                import sys
                from pathlib import Path
                sys.path.insert(0, str(Path(__file__).parent.parent))
                from utils.paths import get_model_path
                model_paths = [
                    str(get_model_path('best_model9.pth')),
                    os.path.join(os.path.dirname(os.path.dirname(__file__)), 'best_model9.pth'),  # Fallback
                    'best_model9.pth',  # Current directory fallback
                ]
            except ImportError:
                project_root = os.path.dirname(os.path.dirname(__file__))
                model_paths = [
                    os.path.join(project_root, 'models', 'best_model9.pth'),
                    os.path.join(project_root, 'best_model9.pth'),  # Fallback
                    'best_model9.pth',  # Current directory fallback
                ]
            
            model_loaded = False
            for model_path in model_paths:
                try:
                    if os.path.exists(model_path):
                        model.load_state_dict(torch.load(model_path, map_location=self.device))
                        model.eval()
                        model_loaded = True
                        logger.info(f"‚úì Model loaded from: {model_path}")
                        break
                except (FileNotFoundError, RuntimeError, KeyError) as e:
                    logger.warning(f"‚ö† Could not load model from {model_path}: {e}")
                    continue
            
            if not model_loaded:
                logger.warning("‚ö† Warning: Model file not found, using untrained model")
                model.eval()
            
            _model_instance = model.to(self.device)
            return _model_instance

    # ============================================================================
    # REVOLUTIONARY NEW FEATURES - v4.0 Ultra-Advanced Detection Methods
    # ============================================================================

    def _detect_error_level_analysis(self, image):
        """
        Error Level Analysis (ELA) - Detects JPEG compression inconsistencies
        Real photos have uniform compression, AI images show anomalies
        """
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        try:
            # Save with high quality
            buffer1 = io.BytesIO()
            image.save(buffer1, 'JPEG', quality=95)
            buffer1.seek(0)
            img_high = Image.open(buffer1)
            
            # Save with lower quality
            buffer2 = io.BytesIO()
            img_high.save(buffer2, 'JPEG', quality=90)
            buffer2.seek(0)
            img_low = Image.open(buffer2)
            
            # Calculate difference
            diff = ImageChops.difference(img_high, img_low)
            diff_array = np.array(diff)
            
            # Calculate ELA score
            ela_score = np.mean(diff_array)
            ela_variance = np.var(diff_array)
            
            # Detect anomalies (AI images have lower ELA variance)
            has_anomaly = ela_variance < 50 or ela_score < 3
            
            return {
                'ela_score': float(ela_score),
                'ela_variance': float(ela_variance),
                'has_compression_anomaly': bool(has_anomaly),
                'compression_uniformity': float(np.std(diff_array))
            }
        except Exception as e:
            logger.warning(f"ELA detection failed: {e}")
            return {'ela_score': 0.0, 'ela_variance': 0.0, 'has_compression_anomaly': False, 'compression_uniformity': 0.0}

    def _detect_double_jpeg_compression(self, image):
        """
        Double JPEG Detection - Real photos are usually compressed once,
        AI images may show multiple compression cycles
        """
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        gray = np.array(image.convert('L')).astype(np.float32)
        
        # Apply DCT to detect JPEG block artifacts
        block_size = 8
        h, w = gray.shape
        
        dct_coefficients = []
        for i in range(0, h - block_size, block_size):
            for j in range(0, w - block_size, block_size):
                block = gray[i:i+block_size, j:j+block_size]
                if block.shape == (block_size, block_size):
                    if SCIPY_AVAILABLE:
                        try:
                            dct_block = dctn(block, norm='ortho')
                        except:
                            dct_block = cv2.dct(block)
                    else:
                        dct_block = cv2.dct(block)
                    dct_coefficients.append(dct_block.flatten())
        
        if not dct_coefficients:
            return {'double_jpeg_score': 0.0, 'has_double_compression': False, 'coefficient_variance': 0.0}
        
        dct_array = np.array(dct_coefficients)
        
        # Analyze coefficient distribution
        coef_variance = np.var(dct_array, axis=0)
        double_jpeg_score = float(np.mean(coef_variance))
        
        # Double compression shows periodic peaks
        has_double_comp = double_jpeg_score > 1000
        
        return {
            'double_jpeg_score': double_jpeg_score,
            'has_double_compression': bool(has_double_comp),
            'coefficient_variance': float(np.mean(coef_variance))
        }

    def _detect_copy_move_forgery(self, image):
        """
        Copy-Move Detection - Detects if parts of image are copied/pasted
        Common in AI-generated images and manipulated photos
        """
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)
        
        # Use SIFT for keypoint detection
        try:
            sift = cv2.SIFT_create()
            keypoints, descriptors = sift.detectAndCompute(gray, None)
            
            if descriptors is None or len(keypoints) < 10:
                return {'copy_move_score': 0.0, 'has_copy_move': False, 'similar_regions': 0, 'total_keypoints': 0}
            
            # Match keypoints to themselves
            bf = cv2.BFMatcher()
            matches = bf.knnMatch(descriptors, descriptors, k=3)
            
            # Find similar regions (excluding self-matches)
            similar_regions = 0
            for match in matches:
                if len(match) >= 2:
                    m, n = match[0], match[1]
                    # If two keypoints are very similar but not the same
                    if m.distance < 0.7 * n.distance and m.queryIdx != m.trainIdx:
                        similar_regions += 1
            
            copy_move_score = similar_regions / max(len(keypoints), 1)
            has_copy_move = copy_move_score > 0.15
            
            return {
                'copy_move_score': float(copy_move_score),
                'has_copy_move': bool(has_copy_move),
                'similar_regions': int(similar_regions),
                'total_keypoints': int(len(keypoints))
            }
        except Exception as e:
            logger.warning(f"Copy-move detection failed: {e}")
            return {'copy_move_score': 0.0, 'has_copy_move': False, 'similar_regions': 0, 'total_keypoints': 0}

    def _detect_gan_fingerprints(self, image):
        """
        GAN Fingerprinting - Detects specific patterns from GAN generators
        Different GANs leave unique 'fingerprints' in generated images
        """
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        gray = np.array(image.convert('L')).astype(np.float32)
        
        # Apply FFT to detect periodic patterns
        f_transform = np.fft.fft2(gray)
        f_shift = np.fft.fftshift(f_transform)
        magnitude = np.abs(f_shift)
        
        # Look for checkerboard artifacts (common in GANs)
        h, w = magnitude.shape
        center_h, center_w = h // 2, w // 2
        
        # Sample high-frequency regions
        high_freq_region = magnitude.copy()
        high_freq_region[center_h-30:center_h+30, center_w-30:center_w+30] = 0
        
        # Detect periodic peaks (GAN artifact)
        peaks = []
        for i in range(4, min(h//2, 100), 4):
            for j in range(4, min(w//2, 100), 4):
                if high_freq_region[center_h+i, center_w+j] > np.mean(high_freq_region) * 3:
                    peaks.append((i, j))
        
        # Check for grid pattern
        if len(peaks) > 10:
            x_coords = [p[0] for p in peaks]
            y_coords = [p[1] for p in peaks]
            x_spacing = np.std(np.diff(sorted(x_coords))) if len(x_coords) > 1 else 0
            y_spacing = np.std(np.diff(sorted(y_coords))) if len(y_coords) > 1 else 0
            
            # Regular spacing indicates GAN fingerprint
            has_fingerprint = (x_spacing < 2 and y_spacing < 2) if x_spacing > 0 else False
        else:
            has_fingerprint = False
            x_spacing = 0
            y_spacing = 0
        
        gan_score = len(peaks) / 100.0
        
        return {
            'gan_fingerprint_detected': bool(has_fingerprint),
            'gan_score': float(min(gan_score, 1.0)),
            'periodic_peaks': int(len(peaks)),
            'grid_regularity': float(1.0 / (x_spacing + y_spacing + 1) if (x_spacing + y_spacing) > 0 else 0)
        }

    def _detect_diffusion_signatures(self, image):
        """
        Diffusion Model Signature Detection - Detects patterns from Stable Diffusion, DALL-E, Midjourney
        Analyzes latent space artifacts and denoising patterns
        """
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        img_array = np.array(image)
        
        # 1. Check for tiling artifacts (common in SD)
        h, w = img_array.shape[:2]
        tile_size = 64
        tile_variances = []
        
        for i in range(0, h - tile_size, tile_size):
            for j in range(0, w - tile_size, tile_size):
                tile = img_array[i:i+tile_size, j:j+tile_size]
                tile_variances.append(np.var(tile))
        
        # Diffusion models have very uniform tile variance
        tile_uniformity = np.std(tile_variances) if tile_variances else 0
        has_tiling = tile_uniformity < 300
        
        # 2. Check for latent space artifacts (512x512, 768x768 common)
        is_common_diffusion_size = (
            (w == 512 and h == 512) or 
            (w == 768 and h == 768) or 
            (w == 1024 and h == 1024) or
            (w % 64 == 0 and h % 64 == 0)
        )
        
        # 3. Analyze color distribution (diffusion models have characteristic color patterns)
        hsv = cv2.cvtColor(img_array, cv2.COLOR_RGB2HSV)
        h_channel = hsv[:,:,0]
        s_channel = hsv[:,:,1]
        
        # Diffusion models often have very smooth hue transitions
        hue_gradient = np.gradient(h_channel.astype(float))
        hue_smoothness = np.mean([np.std(hue_gradient[0]), np.std(hue_gradient[1])])
        
        # 4. Check for denoising patterns
        if SKIMAGE_AVAILABLE:
            try:
                denoised = denoise_tv_chambolle(img_array, weight=0.1)
                denoise_diff = np.mean(np.abs(img_array.astype(float) - denoised))
            except:
                denoise_diff = 0
        else:
            denoise_diff = 0
        
        # Compute diffusion score
        diffusion_score = 0
        if has_tiling:
            diffusion_score += 0.3
        if is_common_diffusion_size:
            diffusion_score += 0.2
        if hue_smoothness < 5:
            diffusion_score += 0.25
        if denoise_diff < 15:
            diffusion_score += 0.25
        
        return {
            'diffusion_signature_detected': bool(diffusion_score > 0.5),
            'diffusion_score': float(diffusion_score),
            'has_tiling_artifacts': bool(has_tiling),
            'is_common_size': bool(is_common_diffusion_size),
            'tile_uniformity': float(tile_uniformity),
            'hue_smoothness': float(hue_smoothness),
            'denoise_difference': float(denoise_diff)
        }

    def _compute_perceptual_hashes(self, image):
        """
        Perceptual Hashing - Generate multiple hashes for reverse image search
        Detects if image is similar to known AI-generated images
        """
        if not IMAGEHASH_AVAILABLE:
            return {
                'average_hash': '',
                'perceptual_hash': '',
                'difference_hash': '',
                'wavelet_hash': '',
                'hash_diversity': 0
            }
        
        try:
            # Compute multiple perceptual hashes
            ahash = str(imagehash.average_hash(image))
            phash = str(imagehash.phash(image))
            dhash = str(imagehash.dhash(image))
            whash = str(imagehash.whash(image))
            
            return {
                'average_hash': ahash,
                'perceptual_hash': phash,
                'difference_hash': dhash,
                'wavelet_hash': whash,
                'hash_diversity': len(set([ahash, phash, dhash, whash]))
            }
        except Exception as e:
            logger.warning(f"Perceptual hashing failed: {e}")
            return {
                'average_hash': '',
                'perceptual_hash': '',
                'difference_hash': '',
                'wavelet_hash': '',
                'hash_diversity': 0
            }

    def _detect_face_manipulation(self, image):
        """
        Face Manipulation Detection - Detects deepfakes and face swaps
        Analyzes facial landmarks and biological signals
        """
        try:
            img_array = np.array(image.convert('RGB'))
            
            # Use Haar Cascade for face detection
            face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
            gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
            faces = face_cascade.detectMultiScale(gray, 1.1, 4)
            
            if len(faces) == 0:
                return {
                    'faces_detected': 0,
                    'has_face_manipulation': False,
                    'manipulation_score': 0.0,
                    'face_details': []
                }
            
            manipulation_indicators = 0
            face_details = []
            
            for (x, y, w, h) in faces:
                face_roi = img_array[y:y+h, x:x+w]
                
                # Check for inconsistent lighting on face
                face_gray = cv2.cvtColor(face_roi, cv2.COLOR_RGB2GRAY)
                lighting_variance = np.var(face_gray)
                
                # Check for unnatural skin texture (too smooth = AI/manipulation)
                laplacian_var = cv2.Laplacian(face_gray, cv2.CV_64F).var()
                is_too_smooth = laplacian_var < 50
                
                # Check for color inconsistency around face edges
                edge_region_top = img_array[max(0,y-10):y, x:x+w] if y > 10 else None
                edge_region_face = face_roi[0:10, :]
                
                edge_color_diff = 0
                if edge_region_top is not None and edge_region_top.size > 0 and edge_region_face.size > 0:
                    edge_color_diff = np.mean(np.abs(
                        np.mean(edge_region_top, axis=(0,1)) - 
                        np.mean(edge_region_face, axis=(0,1))
                    ))
                
                # Unnatural edge discontinuity indicates face swap
                has_edge_discontinuity = edge_color_diff > 15
                
                if is_too_smooth:
                    manipulation_indicators += 1
                if has_edge_discontinuity:
                    manipulation_indicators += 1
                
                face_details.append({
                    'position': [int(x), int(y), int(w), int(h)],
                    'lighting_variance': float(lighting_variance),
                    'texture_score': float(laplacian_var),
                    'is_too_smooth': bool(is_too_smooth),
                    'edge_discontinuity': float(edge_color_diff)
                })
            
            manipulation_score = manipulation_indicators / (len(faces) * 2)
            has_manipulation = manipulation_score > 0.4
            
            return {
                'faces_detected': int(len(faces)),
                'has_face_manipulation': bool(has_manipulation),
                'manipulation_score': float(manipulation_score),
                'face_details': face_details
            }
        except Exception as e:
            logger.warning(f"Face manipulation detection failed: {e}")
            return {
                'faces_detected': 0,
                'has_face_manipulation': False,
                'manipulation_score': 0.0,
                'face_details': []
            }

    def _detect_adversarial_perturbations(self, image):
        """
        Adversarial Perturbation Detection - Detects if image has been
        adversarially modified to fool AI detectors
        """
        if not SCIPY_AVAILABLE:
            return {'has_adversarial_noise': False, 'perturbation_score': 0.0, 'noise_variance': 0.0, 'noise_kurtosis': 0.0}
        
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        img_array = np.array(image).astype(np.float32)
        
        try:
            # Apply high-pass filter to isolate high-frequency noise
            blurred = gaussian_filter(img_array, sigma=1.0)
            high_freq = img_array - blurred
            
            # Adversarial noise has specific statistical properties
            noise_variance = np.var(high_freq)
            noise_kurtosis = stats.kurtosis(high_freq.flatten())
            
            # Check for uniform high-frequency noise (adversarial characteristic)
            is_uniform_noise = noise_variance > 5 and abs(noise_kurtosis) < 2
            
            # Compute perturbation score
            perturbation_score = min(noise_variance / 100.0, 1.0)
            
            return {
                'has_adversarial_noise': bool(is_uniform_noise),
                'perturbation_score': float(perturbation_score),
                'noise_variance': float(noise_variance),
                'noise_kurtosis': float(noise_kurtosis)
            }
        except Exception as e:
            logger.warning(f"Adversarial perturbation detection failed: {e}")
            return {'has_adversarial_noise': False, 'perturbation_score': 0.0, 'noise_variance': 0.0, 'noise_kurtosis': 0.0}

    def _analyze_local_binary_patterns(self, image):
        """
        Advanced LBP Analysis - Texture descriptor that reveals AI generation patterns
        """
        if not SKIMAGE_AVAILABLE:
            return {'lbp_entropy': 0.0, 'lbp_uniformity': 0.0, 'has_artificial_texture': False, 'texture_complexity': 0.0}
        
        try:
            gray = np.array(image.convert('L'))
            
            # Compute LBP
            radius = 3
            n_points = 8 * radius
            lbp = feature.local_binary_pattern(gray, n_points, radius, method='uniform')
            
            # Analyze LBP histogram
            n_bins = int(lbp.max() + 1)
            hist, _ = np.histogram(lbp, bins=n_bins, range=(0, n_bins), density=True)
            
            # AI images have very uniform LBP distributions
            lbp_entropy = -np.sum(hist * np.log2(hist + 1e-10))
            lbp_uniformity = np.std(hist)
            
            # Natural images have higher LBP entropy
            has_artificial_texture = lbp_entropy < 4.0 or lbp_uniformity < 0.02
            
            return {
                'lbp_entropy': float(lbp_entropy),
                'lbp_uniformity': float(lbp_uniformity),
                'has_artificial_texture': bool(has_artificial_texture),
                'texture_complexity': float(lbp_entropy * lbp_uniformity)
            }
        except Exception as e:
            logger.warning(f"LBP analysis failed: {e}")
            return {'lbp_entropy': 0.0, 'lbp_uniformity': 0.0, 'has_artificial_texture': False, 'texture_complexity': 0.0}

    def _detect_splicing_artifacts(self, image):
        """
        Splicing Detection - Detects if multiple images are combined
        Analyzes edge inconsistencies and boundary artifacts
        """
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        img_array = np.array(image)
        gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
        
        # Detect edges
        edges = cv2.Canny(gray, 50, 150)
        
        # Analyze edge connectivity (spliced images have disconnected edges)
        if SKIMAGE_AVAILABLE:
            try:
                labeled_edges = measure.label(edges)
                num_components = np.max(labeled_edges)
                
                # Count edge discontinuities
                h, w = edges.shape
                discontinuities = 0
                
                # Sample grid points
                for i in range(20, h-20, 40):
                    for j in range(20, w-20, 40):
                        local_region = edges[i-20:i+20, j-20:j+20]
                        if np.sum(local_region) > 0:
                            # Check if edges are continuous
                            labeled_local = measure.label(local_region)
                            if np.max(labeled_local) > 3:
                                discontinuities += 1
                
                splicing_score = discontinuities / max(num_components, 1)
                has_splicing = splicing_score > 0.3
            except Exception as e:
                logger.warning(f"Splicing detection failed: {e}")
                splicing_score = 0.0
                has_splicing = False
                num_components = 0
        else:
            splicing_score = 0.0
            has_splicing = False
            num_components = 0
        
        return {
            'splicing_score': float(splicing_score),
            'has_splicing': bool(has_splicing),
            'edge_components': int(num_components) if SKIMAGE_AVAILABLE else 0,
            'edge_density': float(np.sum(edges) / edges.size)
        }

    def _analyze_jpeg_quality_estimate(self, image):
        """
        JPEG Quality Estimation - Estimates compression quality
        AI images often have no compression or unusual quality levels
        """
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        try:
            # Save at multiple quality levels and find best match
            original_array = np.array(image)
            best_quality = 100
            min_difference = float('inf')
            
            for quality in [95, 90, 85, 80, 75, 70]:
                buffer = io.BytesIO()
                image.save(buffer, 'JPEG', quality=quality)
                buffer.seek(0)
                compressed = Image.open(buffer)
                compressed_array = np.array(compressed)
                
                difference = np.mean(np.abs(original_array.astype(float) - compressed_array.astype(float)))
                
                if difference < min_difference:
                    min_difference = difference
                    best_quality = quality
            
            # Very high estimated quality suggests no prior compression (AI image)
            is_uncompressed = best_quality > 92
            
            return {
                'estimated_jpeg_quality': int(best_quality),
                'compression_difference': float(min_difference),
                'appears_uncompressed': bool(is_uncompressed)
            }
        except Exception as e:
            logger.warning(f"JPEG quality estimation failed: {e}")
            return {'estimated_jpeg_quality': 100, 'compression_difference': 0.0, 'appears_uncompressed': True}

    def _analyze_benford_law(self, image):
        """
        Benford's Law Analysis - Natural images follow Benford's distribution
        AI images often deviate from this natural pattern
        """
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        try:
            img_array = np.array(image)
            gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
            
            # Get first digits of pixel values
            pixel_values = gray.flatten()
            first_digits = [int(str(int(v))[0]) for v in pixel_values if v > 0]
            
            if len(first_digits) < 100:
                return {'benford_score': 0.0, 'follows_benford': False}
            
            # Count digit frequencies
            digit_counts = Counter(first_digits)
            total = len(first_digits)
            
            # Expected Benford's distribution
            benford_expected = {d: total * np.log10(1 + 1/d) for d in range(1, 10)}
            
            # Calculate chi-square statistic
            chi_square = 0
            for d in range(1, 10):
                observed = digit_counts.get(d, 0)
                expected = benford_expected[d]
                if expected > 0:
                    chi_square += (observed - expected) ** 2 / expected
            
            # Benford's law: natural images should have low chi-square
            follows_benford = chi_square < 50
            benford_score = 1.0 - min(chi_square / 200.0, 1.0)
            
            return {
                'benford_score': float(benford_score),
                'follows_benford': bool(follows_benford),
                'chi_square': float(chi_square)
            }
        except Exception as e:
            logger.warning(f"Benford's law analysis failed: {e}")
            return {'benford_score': 0.0, 'follows_benford': False, 'chi_square': 0.0}

    # ============================================================================
    # ENHANCED EXISTING METHODS
    # ============================================================================

    def _analyze_metadata(self, image):
        """Enhanced metadata analysis"""
        metadata = {
            'format': str(image.format) if image.format else 'Unknown',
            'mode': str(image.mode),
            'has_exif': bool(image.getexif()),
            'exif_data': {},
            'suspicious_indicators': []
        }
        
        exif = image.getexif()
        if exif:
            metadata['exif_data'] = {
                'camera_make': str(exif.get(271, 'Unknown')),
                'camera_model': str(exif.get(272, 'Unknown')),
                'software': str(exif.get(305, 'Unknown')),
                'datetime': str(exif.get(306, 'Unknown'))
            }
            
            # Check for AI generation software signatures
            software = str(exif.get(305, '')).lower()
            ai_tools = ['midjourney', 'dall-e', 'dalle', 'stable diffusion', 'stablediffusion', 
                       'playground', 'leonardo', 'firefly', 'craiyon', 'deepai']
            if any(ai_tool in software for ai_tool in ai_tools):
                metadata['suspicious_indicators'].append('AI generation software detected in EXIF')
        else:
            metadata['suspicious_indicators'].append('No EXIF data (common in AI images)')
        
        return metadata

    def _analyze_compression_artifacts(self, image):
        """Enhanced compression analysis"""
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        img_array = np.array(image)
        gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
        
        # Multiple edge detection methods
        edges_canny = cv2.Canny(gray, 100, 200)
        edges_sobel = cv2.Sobel(gray, cv2.CV_64F, 1, 1, ksize=3)
        
        edge_density = np.sum(edges_canny > 0) / edges_canny.size
        edge_variance = np.var(edges_sobel)
        
        # Block artifact detection (8x8 blocks common in JPEG)
        h, w = gray.shape
        block_scores = []
        for i in range(0, h-8, 8):
            for j in range(0, w-8, 8):
                block = gray[i:i+8, j:j+8]
                block_scores.append(np.std(block))
        
        block_consistency = np.std(block_scores) if block_scores else 0
        
        # DCT coefficient analysis for JPEG detection
        dct_score = 0
        if h > 64 and w > 64:
            sample = gray[h//2:h//2+64, w//2:w//2+64].astype(np.float32)
            dct = cv2.dct(sample)
            dct_score = float(np.std(dct))
        
        return {
            'edge_density': float(edge_density),
            'edge_variance': float(edge_variance),
            'block_consistency': float(block_consistency),
            'dct_score': float(dct_score),
            'has_compression': bool(edge_density > 0.05),
            'natural_compression': bool(block_consistency > 5.0 and dct_score > 100)
        }

    def _analyze_frequency_domain(self, image):
        """Advanced frequency domain analysis - 2026 enhanced techniques"""
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        gray = np.array(image.convert('L'))
        
        # Apply FFT with windowing to reduce edge artifacts
        h, w = gray.shape
        window = np.outer(np.hanning(h), np.hanning(w))
        gray_windowed = gray * window
        
        # Apply FFT
        f_transform = np.fft.fft2(gray_windowed)
        f_shift = np.fft.fftshift(f_transform)
        magnitude = np.abs(f_shift)
        phase = np.angle(f_shift)
        
        rows, cols = gray.shape
        crow, ccol = rows // 2, cols // 2
        
        # Enhanced frequency band analysis with multiple scales
        high_freq = magnitude.copy()
        high_freq[crow-30:crow+30, ccol-30:ccol+30] = 0
        high_freq_energy = np.sum(high_freq) / np.sum(magnitude) if np.sum(magnitude) > 0 else 0
        
        # Low frequency analysis
        low_freq = np.zeros_like(magnitude)
        low_freq[crow-30:crow+30, ccol-30:ccol+30] = magnitude[crow-30:crow+30, ccol-30:ccol+30]
        low_freq_energy = np.sum(low_freq) / np.sum(magnitude) if np.sum(magnitude) > 0 else 0
        
        # Mid frequency (real photos have good mid-frequency content)
        mid_freq = magnitude.copy()
        mid_freq[crow-15:crow+15, ccol-15:ccol+15] = 0
        mid_freq[crow-45:crow+45, ccol-45:ccol+45] = 0
        mid_freq_energy = np.sum(mid_freq) / np.sum(magnitude) if np.sum(magnitude) > 0 else 0
        
        # NEW 2026: Phase coherence analysis (AI images have different phase patterns)
        phase_coherence = np.std(phase)
        
        # NEW 2026: Radial frequency analysis (check for circular patterns)
        y, x = np.ogrid[:rows, :cols]
        center_y, center_x = rows // 2, cols // 2
        r = np.sqrt((x - center_x)**2 + (y - center_y)**2)
        r_max = min(rows, cols) // 2
        
        # Analyze frequency distribution by radius
        radial_profile = []
        for radius in range(0, r_max, 5):
            mask = (r >= radius) & (r < radius + 5)
            if np.sum(mask) > 0:
                radial_profile.append(np.mean(magnitude[mask]))
        
        radial_variance = np.var(radial_profile) if len(radial_profile) > 1 else 0
        
        # AI images often have unusual frequency distribution
        freq_ratio = high_freq_energy / low_freq_energy if low_freq_energy > 0 else 0
        
        # NEW 2026: Enhanced AI signature detection
        ai_signature_score = 0
        if freq_ratio < 0.12:
            ai_signature_score += 1
        if phase_coherence < 1.5:  # Low phase coherence indicates AI
            ai_signature_score += 1
        if radial_variance < 1000:  # Uniform radial distribution indicates AI
            ai_signature_score += 1
        
        return {
            'high_freq_ratio': float(high_freq_energy),
            'low_freq_ratio': float(low_freq_energy),
            'mid_freq_ratio': float(mid_freq_energy),
            'freq_balance': float(freq_ratio),
            'phase_coherence': float(phase_coherence),  # NEW
            'radial_variance': float(radial_variance),  # NEW
            'spectral_anomaly': bool(high_freq_energy < 0.3 or freq_ratio < 0.1),
            'ai_signature': bool(ai_signature_score >= 2)  # Enhanced detection
        }

    def _analyze_texture_consistency(self, image):
        """Enhanced texture pattern analysis - 2026 Advanced Techniques"""
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        img_array = np.array(image)
        h, w = img_array.shape[:2]
        patch_size = 32
        
        variances = []
        means = []
        local_patterns = []
        gradient_magnitudes = []
        lbp_patterns = []
        
        # Multi-scale texture analysis
        for scale in [1, 2]:
            current_patch_size = patch_size // scale
            if current_patch_size < 8:
                break
                
            for i in range(0, h - current_patch_size, current_patch_size):
                for j in range(0, w - current_patch_size, current_patch_size):
                    patch = img_array[i:i+current_patch_size, j:j+current_patch_size]
                    variances.append(np.var(patch))
                    means.append(np.mean(patch))
                    
                    # Local binary pattern approximation
                    gray_patch = cv2.cvtColor(patch, cv2.COLOR_RGB2GRAY)
                    local_patterns.append(np.std(gray_patch))
                    
                    # NEW 2026: Gradient magnitude analysis
                    grad_x = cv2.Sobel(gray_patch, cv2.CV_64F, 1, 0, ksize=3)
                    grad_y = cv2.Sobel(gray_patch, cv2.CV_64F, 0, 1, ksize=3)
                    gradient_mag = np.sqrt(grad_x**2 + grad_y**2)
                    gradient_magnitudes.append(np.mean(gradient_mag))
                    
                    # NEW 2026: Local Binary Pattern (LBP) for texture description
                    try:
                        # Simplified LBP calculation
                        center = gray_patch[1:-1, 1:-1]
                        neighbors = np.array([
                            gray_patch[0:-2, 0:-2],  # Top-left
                            gray_patch[0:-2, 1:-1],  # Top
                            gray_patch[0:-2, 2:],    # Top-right
                            gray_patch[1:-1, 2:],    # Right
                            gray_patch[2:, 2:],      # Bottom-right
                            gray_patch[2:, 1:-1],   # Bottom
                            gray_patch[2:, 0:-2],   # Bottom-left
                            gray_patch[1:-1, 0:-2]  # Left
                        ])
                        lbp = np.sum((neighbors > center) * (2 ** np.arange(8).reshape(8, 1, 1)), axis=0)
                        lbp_patterns.append(np.std(lbp))
                    except:
                        pass
        
        texture_consistency = np.std(variances) if variances else 0
        pattern_uniformity = np.std(local_patterns) if local_patterns else 0
        gradient_uniformity = np.std(gradient_magnitudes) if gradient_magnitudes else 0
        lbp_uniformity = np.std(lbp_patterns) if lbp_patterns else 0
        
        # NEW 2026: Enhanced AI detection with multiple texture metrics
        is_too_uniform = texture_consistency < 400 and pattern_uniformity < 8
        has_unnatural_gradients = gradient_uniformity < 5.0 if gradient_magnitudes else False
        has_artificial_lbp = lbp_uniformity < 2.0 if lbp_patterns else False
        
        # Combined texture anomaly score
        texture_anomaly_score = 0
        if is_too_uniform:
            texture_anomaly_score += 1
        if has_unnatural_gradients:
            texture_anomaly_score += 1
        if has_artificial_lbp:
            texture_anomaly_score += 1
        
        return {
            'texture_consistency': float(texture_consistency),
            'pattern_uniformity': float(pattern_uniformity),
            'gradient_uniformity': float(gradient_uniformity),
            'lbp_uniformity': float(lbp_uniformity),
            'texture_anomaly_score': int(texture_anomaly_score),
            'uniform_texture': bool(is_too_uniform),
            'variance_range': float(np.max(variances) - np.min(variances)) if variances else 0.0,
            'ai_smoothness': bool(texture_anomaly_score >= 2)  # Enhanced detection
        }

    def _analyze_color_distribution(self, image):
        """Deep color analysis with AI detection patterns - 2026 Enhanced"""
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        stat = ImageStat.Stat(image)
        img_array = np.array(image)
        
        # Histogram analysis - Enhanced
        hist_r = image.histogram()[0:256]
        hist_g = image.histogram()[256:512]
        hist_b = image.histogram()[512:768]
        
        def calc_entropy(hist):
            hist = np.array(hist) / sum(hist) if sum(hist) > 0 else np.array(hist)
            hist = hist[hist > 0]
            return -np.sum(hist * np.log2(hist)) if len(hist) > 0 else 0
        
        entropy = (calc_entropy(hist_r) + calc_entropy(hist_g) + calc_entropy(hist_b)) / 3
        
        # NEW 2026: Histogram smoothness (AI images have smoother histograms)
        hist_smoothness_r = np.std(np.diff(hist_r))
        hist_smoothness_g = np.std(np.diff(hist_g))
        hist_smoothness_b = np.std(np.diff(hist_b))
        avg_hist_smoothness = (hist_smoothness_r + hist_smoothness_g + hist_smoothness_b) / 3
        
        # Color banding detection (common in AI images) - Enhanced
        unique_colors = len(np.unique(img_array.reshape(-1, 3), axis=0))
        total_pixels = img_array.shape[0] * img_array.shape[1]
        color_diversity = unique_colors / total_pixels if total_pixels > 0 else 0
        
        # NEW 2026: Color clustering analysis (AI images have fewer distinct color clusters)
        from sklearn.cluster import KMeans
        try:
            # Sample pixels for faster clustering
            sample_size = min(10000, total_pixels)
            indices = np.random.choice(total_pixels, sample_size, replace=False)
            sample_colors = img_array.reshape(-1, 3)[indices]
            
            # Find optimal number of clusters
            kmeans_5 = KMeans(n_clusters=5, random_state=42, n_init=10)
            kmeans_5.fit(sample_colors)
            cluster_variance_5 = np.var(kmeans_5.inertia_)
            
            kmeans_10 = KMeans(n_clusters=10, random_state=42, n_init=10)
            kmeans_10.fit(sample_colors)
            cluster_variance_10 = np.var(kmeans_10.inertia_)
            
            # AI images have lower cluster variance (more uniform clusters)
            has_uniform_clusters = bool(cluster_variance_5 < 1000 and cluster_variance_10 < 2000)
        except:
            has_uniform_clusters = False
            cluster_variance_5 = 0
            cluster_variance_10 = 0
        
        # Channel correlation (AI images often have unusual correlations) - Enhanced
        r_channel = img_array[:,:,0].flatten()
        g_channel = img_array[:,:,1].flatten()
        b_channel = img_array[:,:,2].flatten()
        
        rg_corr = np.corrcoef(r_channel, g_channel)[0, 1]
        rb_corr = np.corrcoef(r_channel, b_channel)[0, 1]
        gb_corr = np.corrcoef(g_channel, b_channel)[0, 1]
        
        avg_correlation = (abs(rg_corr) + abs(rb_corr) + abs(gb_corr)) / 3
        
        # NEW 2026: Color space analysis (HSV, LAB)
        hsv = cv2.cvtColor(img_array, cv2.COLOR_RGB2HSV)
        h_channel = hsv[:,:,0].flatten()
        s_channel = hsv[:,:,1].flatten()
        v_channel = hsv[:,:,2].flatten()
        
        # Saturation distribution (AI images often have unnatural saturation)
        saturation_entropy = calc_entropy(np.histogram(s_channel, bins=50)[0])
        saturation_uniform = bool(saturation_entropy < 4.0)  # Too uniform = AI
        
        # NEW 2026: Color gradient analysis
        color_gradients = []
        for i in range(0, img_array.shape[0]-1, 10):
            for j in range(0, img_array.shape[1]-1, 10):
                grad = np.linalg.norm(img_array[i+1, j] - img_array[i, j])
                color_gradients.append(grad)
        
        gradient_variance = np.var(color_gradients) if color_gradients else 0
        has_uniform_gradients = bool(gradient_variance < 100)  # Too uniform = AI
        
        return {
            'entropy': float(entropy),
            'histogram_smoothness': float(avg_hist_smoothness),
            'color_diversity': float(color_diversity),
            'unique_colors': int(unique_colors),
            'channel_correlation': float(avg_correlation),
            'cluster_variance_5': float(cluster_variance_5),
            'cluster_variance_10': float(cluster_variance_10),
            'saturation_entropy': float(saturation_entropy),
            'color_gradient_variance': float(gradient_variance),
            'color_range': [[int(x[0]), int(x[1])] for x in stat.extrema],
            'mean_colors': [float(x) for x in stat.mean],
            'std_colors': [float(x) for x in stat.stddev],
            'low_entropy': bool(entropy < 5.5),
            'suspicious_correlation': bool(avg_correlation > 0.88),
            'color_banding': bool(color_diversity < 0.25),
            'uniform_color_clusters': bool(has_uniform_clusters),
            'unnatural_saturation': bool(saturation_uniform),
            'uniform_color_gradients': bool(has_uniform_gradients)
        }

    def _analyze_noise_patterns(self, image):
        """Advanced noise analysis for AI detection - 2026 Enhanced"""
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        gray = np.array(image.convert('L'))
        
        # Apply high-pass filter to isolate noise (multiple kernel sizes)
        noise_results = []
        for kernel_size in [3, 5, 7]:
            blurred = cv2.GaussianBlur(gray, (kernel_size, kernel_size), 0)
            noise = cv2.subtract(gray, blurred)
            noise_results.append(noise)
        
        # Use median for robustness
        noise = np.median(noise_results, axis=0).astype(np.float32)
        
        noise_level = float(np.std(noise))
        noise_mean = float(np.mean(noise))
        
        # NEW 2026: Gaussian distribution test (natural noise is Gaussian)
        hist, bins = np.histogram(noise.flatten(), bins=50, range=(-50, 50))
        hist_normalized = hist / np.sum(hist) if np.sum(hist) > 0 else hist
        
        # Compare to expected Gaussian distribution
        is_gaussian = False
        chi_square = 0
        if SCIPY_AVAILABLE:
            try:
                # Fit Gaussian to noise distribution
                mu, sigma = stats.norm.fit(noise.flatten())
                expected_gaussian = stats.norm.pdf(bins[:-1], mu, sigma)
                expected_gaussian = expected_gaussian / np.sum(expected_gaussian) if np.sum(expected_gaussian) > 0 else expected_gaussian
                
                # Chi-square test for Gaussian distribution
                chi_square = np.sum((hist_normalized - expected_gaussian)**2 / (expected_gaussian + 1e-10))
                is_gaussian = chi_square < 0.1  # Natural noise should be close to Gaussian
            except:
                is_gaussian = False
                chi_square = 0
        else:
            # Fallback: Simple normality test using skewness and kurtosis
            noise_flat = noise.flatten()
            skewness = np.mean(((noise_flat - np.mean(noise_flat)) / (np.std(noise_flat) + 1e-10))**3)
            kurtosis = np.mean(((noise_flat - np.mean(noise_flat)) / (np.std(noise_flat) + 1e-10))**4) - 3
            # Gaussian has skewness ‚âà 0 and kurtosis ‚âà 0
            is_gaussian = abs(skewness) < 0.5 and abs(kurtosis) < 1.0
            chi_square = abs(skewness) + abs(kurtosis)
        
        noise_distribution = np.std(hist)
        
        # Check for noise uniformity across image (multi-region analysis)
        h, w = noise.shape
        regions = [
            noise[0:h//3, 0:w//3],           # Top-left
            noise[0:h//3, 2*w//3:w],         # Top-right
            noise[2*h//3:h, 0:w//3],         # Bottom-left
            noise[2*h//3:h, 2*w//3:w],       # Bottom-right
            noise[h//3:2*h//3, w//3:2*w//3]  # Center
        ]
        region_noise = [np.std(r) for r in regions]
        noise_uniformity = np.std(region_noise)
        noise_spatial_variance = np.var(region_noise)
        
        # NEW 2026: Noise autocorrelation (natural noise is uncorrelated)
        noise_autocorr = np.corrcoef(noise[0:-1].flatten(), noise[1:].flatten())[0, 1]
        has_correlated_noise = abs(noise_autocorr) > 0.1  # AI noise may be correlated
        
        # Analyze noise frequency characteristics
        noise_fft = np.fft.fft2(noise)
        noise_magnitude = np.abs(noise_fft)
        noise_energy = float(np.sum(noise_magnitude))
        
        # NEW 2026: Noise spectrum analysis
        noise_spectrum_flat = noise_magnitude.flatten()
        noise_spectrum_entropy = -np.sum((noise_spectrum_flat / np.sum(noise_spectrum_flat) + 1e-10) * 
                                        np.log2(noise_spectrum_flat / np.sum(noise_spectrum_flat) + 1e-10))
        
        return {
            'noise_level': noise_level,
            'noise_mean': noise_mean,
            'noise_distribution': float(noise_distribution),
            'noise_uniformity': float(noise_uniformity),
            'noise_spatial_variance': float(noise_spatial_variance),
            'noise_autocorrelation': float(noise_autocorr),
            'noise_energy': noise_energy,
            'noise_spectrum_entropy': float(noise_spectrum_entropy),
            'is_gaussian_noise': bool(is_gaussian),
            'gaussian_chi_square': float(chi_square),
            'has_natural_noise': bool(5 < noise_level < 30 and is_gaussian),
            'suspiciously_clean': bool(noise_level < 3),
            'artificial_noise': bool(noise_uniformity > 2.5 or has_correlated_noise),
            'unnatural_noise_pattern': bool(not is_gaussian or has_correlated_noise)
        }

    def _detect_ai_artifacts(self, image):
        """Detect specific AI generation artifacts - 2026 Advanced"""
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        img_array = np.array(image)
        artifacts = {}
        
        # 1. Enhanced Pixel repetition (copy-paste artifacts in diffusion models)
        gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
        template_size = 16
        h, w = gray.shape
        repetition_score = 0
        high_similarity_regions = 0
        
        if h > template_size * 4 and w > template_size * 4:
            # Check multiple template locations for better detection
            template_locations = [
                (h//2, w//2),
                (h//4, w//4),
                (3*h//4, 3*w//4),
                (h//4, 3*w//4),
                (3*h//4, w//4)
            ]
            
            for ty, tx in template_locations:
                if ty + template_size < h and tx + template_size < w:
                    template = gray[ty:ty+template_size, tx:tx+template_size]
                    result = cv2.matchTemplate(gray, template, cv2.TM_CCOEFF_NORMED)
                    matches = np.where(result > 0.92)  # Lower threshold for better detection
                    repetition_score += len(matches[0])
                    high_similarity_regions += len(np.where(result > 0.85)[0])
        
        artifacts['pixel_repetition'] = float(repetition_score / 100)
        artifacts['high_similarity_regions'] = float(high_similarity_regions / 1000)
        artifacts['has_pixel_repetition'] = bool(repetition_score > 50 or high_similarity_regions > 200)
        
        # 2. Unnatural smoothness in faces/skin (GAN artifacts)
        laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
        artifacts['smoothness_score'] = float(laplacian_var)
        artifacts['unnatural_smoothness'] = bool(laplacian_var < 40)
        
        # 3. Grid patterns (common in diffusion models)
        fft = np.fft.fft2(gray)
        fft_shift = np.fft.fftshift(fft)
        magnitude_spectrum = np.abs(fft_shift)
        
        # Look for regular grid patterns
        center = np.array(magnitude_spectrum.shape) // 2
        grid_score = 0
        for offset in [8, 16, 32, 64]:
            if center[0] + offset < magnitude_spectrum.shape[0] and center[1] + offset < magnitude_spectrum.shape[1]:
                grid_score += magnitude_spectrum[center[0] + offset, center[1]]
                grid_score += magnitude_spectrum[center[0], center[1] + offset]
        
        artifacts['grid_pattern_score'] = float(grid_score / 1000000)
        artifacts['has_grid_pattern'] = bool(grid_score > 6000000)
        
        # 4. Chromatic aberration (real cameras have it, AI often doesn't)
        r_channel = img_array[:,:,0]
        b_channel = img_array[:,:,2]
        edge_r = cv2.Canny(r_channel, 100, 200)
        edge_b = cv2.Canny(b_channel, 100, 200)
        chromatic_diff = float(np.sum(np.abs(edge_r.astype(float) - edge_b.astype(float))))
        artifacts['chromatic_aberration'] = chromatic_diff
        artifacts['has_chromatic_aberration'] = bool(chromatic_diff > 1000)
        
        # NEW 2026: Advanced Artifact Detection
        # 5. Edge consistency analysis (AI images have inconsistent edges)
        edges_full = cv2.Canny(gray, 50, 150)
        edge_density = np.sum(edges_full > 0) / edges_full.size
        
        # Analyze edge direction distribution
        grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
        grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
        edge_directions = np.arctan2(grad_y, grad_x) * 180 / np.pi
        edge_directions = edge_directions[edges_full > 0]
        
        if len(edge_directions) > 100:
            direction_hist, _ = np.histogram(edge_directions, bins=36, range=(-180, 180))
            direction_uniformity = np.std(direction_hist)
            artifacts['edge_direction_uniformity'] = float(direction_uniformity)
            artifacts['inconsistent_edges'] = bool(direction_uniformity < 50)  # Too uniform = AI
        else:
            artifacts['edge_direction_uniformity'] = 0.0
            artifacts['inconsistent_edges'] = False
        
        # 6. Local contrast analysis (AI images often have unnatural contrast)
        local_contrast = cv2.Laplacian(gray, cv2.CV_64F).var()
        artifacts['local_contrast'] = float(local_contrast)
        artifacts['unnatural_contrast'] = bool(local_contrast < 20 or local_contrast > 500)
        
        # 7. Histogram analysis for compression artifacts
        hist = cv2.calcHist([gray], [0], None, [256], [0, 256])
        hist_smoothness = np.std(np.diff(hist.flatten()))
        artifacts['histogram_smoothness'] = float(hist_smoothness)
        artifacts['unnatural_histogram'] = bool(hist_smoothness < 5)  # Too smooth = AI
        
        # 8. Block artifact detection (JPEG compression vs AI generation)
        block_size = 8
        block_variances = []
        for i in range(0, h - block_size, block_size):
            for j in range(0, w - block_size, block_size):
                block = gray[i:i+block_size, j:j+block_size]
                block_variances.append(np.var(block))
        
        block_variance_std = np.std(block_variances) if block_variances else 0
        artifacts['block_variance_std'] = float(block_variance_std)
        artifacts['unnatural_block_pattern'] = bool(block_variance_std < 50)  # Too uniform = AI
        
        return artifacts

    def _detect_logos_and_watermarks(self, image):
        """
        Modern 2026 logo and watermark detection
        Detects AI service logos, watermarks, and text indicators
        """
        logo_detections = {
            'detected_logos': [],
            'detected_text': [],
            'watermark_score': 0.0,
            'ai_service_detected': None,
            'confidence': 0.0
        }
        
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        img_array = np.array(image)
        gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
        h, w = gray.shape
        
        # 1. OCR Text Detection for watermarks (corners and edges)
        if OCR_AVAILABLE:
            try:
                # Check corners and edges where watermarks typically appear
                regions = [
                    (0, 0, min(w//4, 200), min(h//4, 200)),  # Top-left
                    (w - min(w//4, 200), 0, min(w//4, 200), min(h//4, 200)),  # Top-right
                    (0, h - min(h//4, 200), min(w//4, 200), min(h//4, 200)),  # Bottom-left
                    (w - min(w//4, 200), h - min(h//4, 200), min(w//4, 200), min(h//4, 200)),  # Bottom-right
                ]
                
                # Also check center for overlays
                center_x, center_y = w // 2, h // 2
                regions.append((center_x - 100, center_y - 50, 200, 100))
                
                all_text = []
                for x, y, rw, rh in regions:
                    if x >= 0 and y >= 0 and x + rw <= w and y + rh <= h:
                        roi = gray[y:y+rh, x:x+rw]
                        # Enhance contrast for better OCR
                        roi_enhanced = cv2.convertScaleAbs(roi, alpha=2.0, beta=30)
                        try:
                            text = pytesseract.image_to_string(roi_enhanced, config='--psm 7').strip().lower()
                            if text:
                                all_text.append(text)
                        except:
                            pass
                
                # Check detected text against AI service indicators - Enhanced matching
                for service, keywords in self.ai_service_indicators.items():
                    for text in all_text:
                        text_lower = text.lower()
                        for keyword in keywords:
                            keyword_lower = keyword.lower()
                            # More flexible matching (partial word matches, common variations)
                            if (keyword_lower in text_lower or 
                                text_lower in keyword_lower or
                                any(word in text_lower for word in keyword_lower.split()) or
                                keyword_lower.replace(' ', '') in text_lower.replace(' ', '')):
                                logo_detections['detected_logos'].append(service.upper())
                                logo_detections['detected_text'].append(text)
                                logo_detections['ai_service_detected'] = service
                                logo_detections['watermark_score'] = 1.0
                                logo_detections['confidence'] = 1.0
                                logger.warning(f"üö® CRITICAL: Detected AI service watermark: {service.upper()} (text: {text[:50]})")
                                break
                        if logo_detections['ai_service_detected']:
                            break
                    if logo_detections['ai_service_detected']:
                        break
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è OCR watermark detection failed: {e}")
        
        # 2. Visual Logo Detection using template matching and pattern recognition
        # Check for common watermark patterns (semi-transparent overlays)
        try:
            # Convert to different color spaces for better detection
            hsv = cv2.cvtColor(img_array, cv2.COLOR_RGB2HSV)
            
            # Look for semi-transparent watermark regions (often have specific brightness/contrast)
            # Watermarks are typically in corners with different characteristics
            corners = [
                gray[0:min(100, h//4), 0:min(200, w//4)],  # Top-left
                gray[0:min(100, h//4), max(0, w-min(200, w//4)):w],  # Top-right
                gray[max(0, h-min(100, h//4)):h, 0:min(200, w//4)],  # Bottom-left
                gray[max(0, h-min(100, h//4)):h, max(0, w-min(200, w//4)):w],  # Bottom-right
            ]
            
            for corner in corners:
                if corner.size > 0:
                    # Watermarks often have specific variance patterns
                    corner_var = np.var(corner)
                    corner_mean = np.mean(corner)
                    
                    # Semi-transparent watermarks create specific patterns
                    if 20 < corner_var < 800 and (corner_mean < 50 or corner_mean > 200):
                        logo_detections['watermark_score'] = max(logo_detections['watermark_score'], 0.3)
            
            # Check for geometric patterns common in logos (circles, stars, specific shapes)
            # Gemini logo often has star/sparkle patterns
            edges = cv2.Canny(gray, 50, 150)
            contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            # Look for star-like patterns (Gemini logo characteristic)
            for contour in contours[:20]:  # Check first 20 contours
                if len(contour) >= 5:
                    area = cv2.contourArea(contour)
                    if 50 < area < 5000:  # Reasonable logo size
                        # Approximate contour to polygon
                        epsilon = 0.02 * cv2.arcLength(contour, True)
                        approx = cv2.approxPolyDP(contour, epsilon, True)
                        
                        # Star-like shapes often have 5-10 points
                        if 5 <= len(approx) <= 10:
                            # Check if it's in a corner (typical watermark location)
                            x, y, w_cont, h_cont = cv2.boundingRect(contour)
                            corner_proximity = min(x, y, w - x - w_cont, h - y - h_cont)
                            
                            if corner_proximity < min(w, h) * 0.15:  # Within 15% of edge
                                logo_detections['watermark_score'] = max(logo_detections['watermark_score'], 0.4)
                                if logo_detections['watermark_score'] > 0.7:
                                    logo_detections['detected_logos'].append('STAR_PATTERN')
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Visual logo detection failed: {e}")
        
        # 3. Color-based detection (some AI services use specific color schemes in watermarks)
        try:
            # Check corners for specific color patterns
            corner_regions = [
                img_array[0:min(50, h//8), 0:min(100, w//8)],  # Top-left
                img_array[0:min(50, h//8), max(0, w-min(100, w//8)):w],  # Top-right
            ]
            
            for corner in corner_regions:
                if corner.size > 0:
                    # Some AI services use specific color combinations
                    corner_colors = corner.reshape(-1, 3)
                    unique_colors = len(np.unique(corner_colors, axis=0))
                    
                    # Very limited color palette in corner might indicate watermark
                    if unique_colors < 20 and corner.size > 100:
                        logo_detections['watermark_score'] = max(logo_detections['watermark_score'], 0.2)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Color-based logo detection failed: {e}")
        
        return logo_detections

    def _comprehensive_analysis(self, image):
        """
        Comprehensive multi-factor analysis - v4.0 Ultra-Advanced
        Includes 50+ detection methods for 95%+ accuracy
        """
        width, height = image.size
        aspect_ratio = width / height
        
        if image.mode in ('RGBA', 'P'):
            rgb_image = image.convert('RGB')
        else:
            rgb_image = image
        
        # Run all enhanced analysis functions (existing)
        metadata = self._analyze_metadata(image)
        compression = self._analyze_compression_artifacts(rgb_image)
        frequency = self._analyze_frequency_domain(rgb_image)
        texture = self._analyze_texture_consistency(rgb_image)
        color = self._analyze_color_distribution(rgb_image)
        noise = self._analyze_noise_patterns(rgb_image)
        artifacts = self._detect_ai_artifacts(rgb_image)
        logo_detection = self._detect_logos_and_watermarks(image)
        
        # NEW v4.0: Forensic Analysis Suite
        ela_result = self._detect_error_level_analysis(image)
        double_jpeg = self._detect_double_jpeg_compression(image)
        copy_move = self._detect_copy_move_forgery(image)
        splicing = self._detect_splicing_artifacts(image)
        jpeg_quality = self._analyze_jpeg_quality_estimate(image)
        
        # NEW v4.0: AI Signature Detection
        gan_fingerprint = self._detect_gan_fingerprints(image)
        diffusion_sig = self._detect_diffusion_signatures(image)
        adversarial = self._detect_adversarial_perturbations(image)
        
        # NEW v4.0: Advanced Texture Analysis
        lbp_analysis = self._analyze_local_binary_patterns(image)
        
        # NEW v4.0: Biological Signal Analysis
        face_manip = self._detect_face_manipulation(image)
        
        # NEW v4.0: Perceptual Hashing
        perceptual_hashes = self._compute_perceptual_hashes(image)
        
        # NEW v4.0: Statistical Analysis
        benford_analysis = self._analyze_benford_law(image)
        
        return {
            'dimensions': {
                'width': width,
                'height': height,
                'aspect_ratio': round(aspect_ratio, 2),
                'total_pixels': width * height
            },
            'metadata': metadata,
            'compression': compression,
            'frequency': frequency,
            'texture': texture,
            'color': color,
            'noise': noise,
            'artifacts': artifacts,
            'logo_detection': logo_detection,
            # NEW v4.0: Forensic Analysis
            'forensics': {
                'ela': ela_result,
                'double_jpeg': double_jpeg,
                'copy_move': copy_move,
                'splicing': splicing,
                'jpeg_quality': jpeg_quality
            },
            # NEW v4.0: AI Signatures
            'ai_signatures': {
                'gan_fingerprint': gan_fingerprint,
                'diffusion_signature': diffusion_sig,
                'adversarial_perturbation': adversarial
            },
            # NEW v4.0: Advanced Texture
            'advanced_texture': {
                'lbp': lbp_analysis
            },
            # NEW v4.0: Biometric Analysis
            'biometric': {
                'face_manipulation': face_manip
            },
            # NEW v4.0: Hashes
            'hashes': perceptual_hashes,
            # NEW v4.0: Statistical
            'statistical': {
                'benford_law': benford_analysis
            },
            'file_size_kb': 0
        }

    def _calculate_confidence_factors(self, analysis):
        """Enhanced confidence calculation with balanced weights - 2026 standards"""
        factors = []
        ai_score = 0
        real_score = 0
        
        # CRITICAL: Logo/Watermark Detection - Highest Priority (100% confidence if detected)
        logo_detection = analysis.get('logo_detection', {})
        if logo_detection.get('ai_service_detected'):
            service = logo_detection['ai_service_detected'].upper()
            factors.insert(0, {
                'factor': f"üö® AI SERVICE WATERMARK DETECTED: {service} (100% AI-generated)",
                'impact': 1.0,  # Maximum impact
                'direction': 'ai',
                'severity': 'critical'
            })
            ai_score += 1.0  # Guaranteed AI if logo detected
            logger.warning(f"üö® CRITICAL: {service} watermark detected - Image is 100% AI-generated")
        
        # High watermark score also indicates AI
        elif logo_detection.get('watermark_score', 0) > 0.7:
            factors.append({
                'factor': f"High watermark score detected ({logo_detection['watermark_score']:.2f})",
                'impact': 0.45,
                'direction': 'ai',
                'severity': 'high'
            })
            ai_score += 0.45
        
        elif logo_detection.get('watermark_score', 0) > 0.4:
            factors.append({
                'factor': f"Possible watermark detected ({logo_detection['watermark_score']:.2f})",
                'impact': 0.25,
                'direction': 'ai',
                'severity': 'medium'
            })
            ai_score += 0.25
        
        # ========================================================================
        # NEW v4.0: FORENSIC ANALYSIS FACTORS (HIGH WEIGHT)
        # ========================================================================
        forensics = analysis.get('forensics', {})
        
        # Error Level Analysis (ELA) - Compression anomaly detection
        if forensics.get('ela', {}).get('has_compression_anomaly', False):
            factors.append({
                'factor': f"ELA compression anomaly detected (variance: {forensics.get('ela', {}).get('ela_variance', 0):.1f})",
                'impact': 0.35,
                'direction': 'ai',
                'severity': 'high'
            })
            ai_score += 0.35
        elif forensics.get('ela', {}).get('ela_variance', 100) > 100:
            factors.append({
                'factor': f"Normal compression pattern (ELA variance: {forensics.get('ela', {}).get('ela_variance', 0):.1f})",
                'impact': 0.25,
                'direction': 'real',
                'severity': 'high'
            })
            real_score += 0.25
        
        # Double JPEG Compression
        if forensics.get('double_jpeg', {}).get('has_double_compression', False):
            factors.append({
                'factor': f"Double JPEG compression detected (score: {forensics.get('double_jpeg', {}).get('double_jpeg_score', 0):.1f})",
                'impact': 0.25,
                'direction': 'ai',
                'severity': 'high'
            })
            ai_score += 0.25
        
        # Copy-Move Forgery
        if forensics.get('copy_move', {}).get('has_copy_move', False):
            factors.append({
                'factor': f"Copy-move forgery detected (score: {forensics.get('copy_move', {}).get('copy_move_score', 0):.3f})",
                'impact': 0.30,
                'direction': 'ai',
                'severity': 'high'
            })
            ai_score += 0.30
        
        # Splicing Detection
        if forensics.get('splicing', {}).get('has_splicing', False):
            factors.append({
                'factor': f"Image splicing detected (score: {forensics.get('splicing', {}).get('splicing_score', 0):.3f})",
                'impact': 0.25,
                'direction': 'ai',
                'severity': 'high'
            })
            ai_score += 0.25
        
        # JPEG Quality - Uncompressed images suggest AI
        jpeg_quality = forensics.get('jpeg_quality', {})
        if jpeg_quality.get('appears_uncompressed', False):
            factors.append({
                'factor': f"Appears uncompressed (estimated quality: {jpeg_quality.get('estimated_jpeg_quality', 100)})",
                'impact': 0.20,
                'direction': 'ai',
                'severity': 'medium'
            })
            ai_score += 0.20
        elif jpeg_quality.get('estimated_jpeg_quality', 100) < 90:
            factors.append({
                'factor': f"Natural JPEG compression detected (quality: {jpeg_quality.get('estimated_jpeg_quality', 0)})",
                'impact': 0.20,
                'direction': 'real',
                'severity': 'medium'
            })
            real_score += 0.20
        
        # ========================================================================
        # NEW v4.0: AI SIGNATURE DETECTION (HIGHEST WEIGHT)
        # ========================================================================
        ai_signatures = analysis.get('ai_signatures', {})
        
        # GAN Fingerprinting
        gan_fp = ai_signatures.get('gan_fingerprint', {})
        if gan_fp.get('gan_fingerprint_detected', False):
            factors.append({
                'factor': f"GAN fingerprint detected (score: {gan_fp.get('gan_score', 0):.3f}, peaks: {gan_fp.get('periodic_peaks', 0)})",
                'impact': 0.45,
                'direction': 'ai',
                'severity': 'critical'
            })
            ai_score += 0.45
        
        # Diffusion Model Signatures
        diffusion = ai_signatures.get('diffusion_signature', {})
        if diffusion.get('diffusion_signature_detected', False):
            factors.append({
                'factor': f"Diffusion model signature detected (score: {diffusion.get('diffusion_score', 0):.3f}, tiling: {diffusion.get('has_tiling_artifacts', False)})",
                'impact': 0.50,
                'direction': 'ai',
                'severity': 'critical'
            })
            ai_score += 0.50
        elif diffusion.get('is_common_size', False):
            factors.append({
                'factor': f"Common diffusion model size detected ({analysis.get('dimensions', {}).get('width', 0)}x{analysis.get('dimensions', {}).get('height', 0)})",
                'impact': 0.15,
                'direction': 'ai',
                'severity': 'medium'
            })
            ai_score += 0.15
        
        # Adversarial Perturbations
        adversarial = ai_signatures.get('adversarial_perturbation', {})
        if adversarial.get('has_adversarial_noise', False):
            factors.append({
                'factor': f"Adversarial perturbation detected (score: {adversarial.get('perturbation_score', 0):.3f})",
                'impact': 0.20,
                'direction': 'ai',
                'severity': 'high'
            })
            ai_score += 0.20
        
        # ========================================================================
        # NEW v4.0: ADVANCED TEXTURE ANALYSIS
        # ========================================================================
        advanced_texture = analysis.get('advanced_texture', {})
        lbp = advanced_texture.get('lbp', {})
        if lbp.get('has_artificial_texture', False):
            factors.append({
                'factor': f"Artificial texture pattern (LBP entropy: {lbp.get('lbp_entropy', 0):.2f}, uniformity: {lbp.get('lbp_uniformity', 0):.4f})",
                'impact': 0.22,
                'direction': 'ai',
                'severity': 'high'
            })
            ai_score += 0.22
        
        # ========================================================================
        # NEW v4.0: BIOLOGICAL SIGNAL ANALYSIS (Deepfake Detection)
        # ========================================================================
        biometric = analysis.get('biometric', {})
        face_manip = biometric.get('face_manipulation', {})
        if face_manip.get('faces_detected', 0) > 0:
            if face_manip.get('has_face_manipulation', False):
                factors.append({
                    'factor': f"Face manipulation detected ({face_manip.get('faces_detected', 0)} face(s), score: {face_manip.get('manipulation_score', 0):.3f})",
                    'impact': 0.30,
                    'direction': 'ai',
                    'severity': 'high'
                })
                ai_score += 0.30
            else:
                factors.append({
                    'factor': f"Natural face detected ({face_manip.get('faces_detected', 0)} face(s))",
                    'impact': 0.30,
                    'direction': 'real',
                    'severity': 'high'
                })
                real_score += 0.30
        
        # ========================================================================
        # NEW v4.0: STATISTICAL ANALYSIS
        # ========================================================================
        statistical = analysis.get('statistical', {})
        benford = statistical.get('benford_law', {})
        if not benford.get('follows_benford', True):
            factors.append({
                'factor': f"Benford's Law deviation (score: {benford.get('benford_score', 0):.3f}, œá¬≤: {benford.get('chi_square', 0):.1f})",
                'impact': 0.15,
                'direction': 'ai',
                'severity': 'medium'
            })
            ai_score += 0.15
        elif benford.get('benford_score', 0) > 0.7:
            factors.append({
                'factor': f"Follows Benford's Law (score: {benford.get('benford_score', 0):.3f})",
                'impact': 0.15,
                'direction': 'real',
                'severity': 'medium'
            })
            real_score += 0.15
        
        # Metadata factors - STRONG INDICATOR
        if not analysis['metadata']['has_exif']:
            factors.append({
                'factor': 'Missing EXIF metadata',
                'impact': 0.05,
                'direction': 'ai',
                'severity': 'low'
            })
            ai_score += 0.05
        else:
            exif_data = analysis['metadata']['exif_data']
            if exif_data.get('camera_make') != 'Unknown' or exif_data.get('camera_model') != 'Unknown':
                factors.append({
                    'factor': f"Camera EXIF data: {exif_data.get('camera_make', '')} {exif_data.get('camera_model', '')}",
                    'impact': 0.25,
                    'direction': 'real',
                    'severity': 'high'
                })
                real_score += 0.25
            else:
                factors.append({
                    'factor': 'EXIF metadata present',
                    'impact': 0.15,
                    'direction': 'real',
                    'severity': 'medium'
                })
                real_score += 0.15
        
        # Check for AI software in EXIF
        if analysis['metadata']['suspicious_indicators']:
            for indicator in analysis['metadata']['suspicious_indicators']:
                if 'AI generation software' in indicator:
                    factors.append({
                        'factor': indicator,
                        'impact': 0.30,
                        'direction': 'ai',
                        'severity': 'high'
                    })
                    ai_score += 0.30
        
        # Noise analysis - MOST RELIABLE INDICATOR FOR REAL PHOTOS - Enhanced 2026
        if analysis['noise']['suspiciously_clean']:
            factors.append({
                'factor': 'Extremely low noise level (AI characteristic)',
                'impact': 0.25,  # Increased
                'direction': 'ai',
                'severity': 'high'
            })
            ai_score += 0.25
        elif analysis['noise'].get('unnatural_noise_pattern', False):
            noise_details = []
            if not analysis['noise'].get('is_gaussian_noise', True):
                noise_details.append('non-Gaussian distribution')
            if analysis['noise'].get('noise_autocorrelation', 0) > 0.1:
                noise_details.append('correlated noise')
            detail_str = f" ({', '.join(noise_details)})" if noise_details else ""
            factors.append({
                'factor': f'Unnatural noise pattern detected{detail_str}',
                'impact': 0.20,
                'direction': 'ai',
                'severity': 'high'
            })
            ai_score += 0.20
        elif analysis['noise']['has_natural_noise']:
            noise_details = []
            if analysis['noise'].get('is_gaussian_noise', False):
                noise_details.append('Gaussian distribution')
            if analysis['noise'].get('noise_spatial_variance', 0) > 1.0:
                noise_details.append('spatial variance')
            detail_str = f" ({', '.join(noise_details)})" if noise_details else ""
            factors.append({
                'factor': f"Natural sensor noise detected (level: {analysis['noise']['noise_level']:.2f}){detail_str}",
                'impact': 0.35,  # Increased - strongest real indicator
                'direction': 'real',
                'severity': 'high'
            })
            real_score += 0.35
        
        # Compression artifacts - STRONG INDICATOR FOR REAL PHOTOS - Enhanced 2026
        if analysis['compression']['natural_compression']:
            comp_details = []
            if analysis['compression'].get('block_boundary_score', 0) > 0.1:
                comp_details.append('block boundaries')
            if analysis['compression'].get('dct_energy', 0) > 5000:
                comp_details.append('DCT energy')
            if analysis['compression'].get('block_consistency', 0) > 5.0:
                comp_details.append('block variance')
            
            detail_str = f" ({', '.join(comp_details)})" if comp_details else ""
            factors.append({
                'factor': f'Natural JPEG compression artifacts detected{detail_str}',
                'impact': 0.30,  # Increased
                'direction': 'real',
                'severity': 'high'
            })
            real_score += 0.30
        elif analysis['compression'].get('ai_compression_pattern', False):
            factors.append({
                'factor': 'Unnatural compression pattern (too uniform, likely AI)',
                'impact': 0.18,
                'direction': 'ai',
                'severity': 'high'
            })
            ai_score += 0.18
        
        # Chromatic aberration - real cameras have it
        if analysis['artifacts'].get('has_chromatic_aberration', False):
            factors.append({
                'factor': 'Chromatic aberration present (camera lens characteristic)',
                'impact': 0.18,
                'direction': 'real',
                'severity': 'medium'
            })
            real_score += 0.18
        
        # Texture factors - Enhanced 2026
        texture_anomaly = analysis['texture'].get('texture_anomaly_score', 0)
        if texture_anomaly >= 2:
            details = []
            if analysis['texture'].get('gradient_uniformity', 10) < 5.0:
                details.append('unnatural gradients')
            if analysis['texture'].get('lbp_uniformity', 5) < 2.0:
                details.append('artificial LBP patterns')
            if analysis['texture']['pattern_uniformity'] < 8:
                details.append('uniform patterns')
            
            detail_str = f" ({', '.join(details)})" if details else ""
            factors.append({
                'factor': f'Artificial texture patterns detected{detail_str}',
                'impact': 0.22,  # Increased
                'direction': 'ai',
                'severity': 'high'
            })
            ai_score += 0.22
        elif analysis['texture']['texture_consistency'] > 1000:
            factors.append({
                'factor': f"Natural texture variation (score: {analysis['texture']['texture_consistency']:.0f})",
                'impact': 0.20,  # Increased
                'direction': 'real',
                'severity': 'medium'
            })
            real_score += 0.20
        
        # Frequency domain analysis - Enhanced 2026
        if analysis['frequency']['ai_signature']:
            freq_details = []
            if analysis['frequency'].get('phase_coherence', 2.0) < 1.5:
                freq_details.append('low phase coherence')
            if analysis['frequency'].get('radial_variance', 2000) < 1000:
                freq_details.append('uniform radial distribution')
            if analysis['frequency']['freq_balance'] < 0.12:
                freq_details.append('abnormal frequency ratio')
            
            detail_str = f" ({', '.join(freq_details)})" if freq_details else ""
            factors.append({
                'factor': f'Abnormal frequency distribution{detail_str}',
                'impact': 0.18,  # Increased impact
                'direction': 'ai',
                'severity': 'high'
            })
            ai_score += 0.18
        elif analysis['frequency']['freq_balance'] > 0.20:
            factors.append({
                'factor': f"Natural frequency distribution (ratio: {analysis['frequency']['freq_balance']:.3f})",
                'impact': 0.15,
                'direction': 'real',
                'severity': 'medium'
            })
            real_score += 0.15
        
        # Color analysis - Enhanced 2026
        if analysis['color']['suspicious_correlation']:
            factors.append({
                'factor': 'Unusual color channel correlation',
                'impact': 0.12,  # Increased
                'direction': 'ai',
                'severity': 'medium'
            })
            ai_score += 0.12
        
        if analysis['color']['color_banding']:
            factors.append({
                'factor': 'Color banding detected (AI artifact)',
                'impact': 0.15,  # Increased
                'direction': 'ai',
                'severity': 'medium'
            })
            ai_score += 0.15
        
        # NEW 2026: Uniform color clusters
        if analysis['color'].get('uniform_color_clusters', False):
            factors.append({
                'factor': f"Uniform color clusters detected (variance: {analysis['color'].get('cluster_variance_5', 0):.0f})",
                'impact': 0.18,
                'direction': 'ai',
                'severity': 'high'
            })
            ai_score += 0.18
        
        # NEW 2026: Unnatural saturation
        if analysis['color'].get('unnatural_saturation', False):
            factors.append({
                'factor': f"Unnatural saturation distribution (entropy: {analysis['color'].get('saturation_entropy', 0):.2f})",
                'impact': 0.12,
                'direction': 'ai',
                'severity': 'medium'
            })
            ai_score += 0.12
        
        # NEW 2026: Uniform color gradients
        if analysis['color'].get('uniform_color_gradients', False):
            factors.append({
                'factor': f"Uniform color gradients (variance: {analysis['color'].get('color_gradient_variance', 0):.1f})",
                'impact': 0.15,
                'direction': 'ai',
                'severity': 'medium'
            })
            ai_score += 0.15
        
        # High entropy is strong real indicator
        if analysis['color']['entropy'] > 7.0:
            factors.append({
                'factor': f"High color entropy (value: {analysis['color']['entropy']:.2f} bits)",
                'impact': 0.22,  # Increased
                'direction': 'real',
                'severity': 'high'
            })
            real_score += 0.22
        elif analysis['color']['entropy'] > 6.0:
            factors.append({
                'factor': f"Good color entropy (value: {analysis['color']['entropy']:.2f} bits)",
                'impact': 0.15,
                'direction': 'real',
                'severity': 'medium'
            })
            real_score += 0.15
        
        # AI artifacts detection - Enhanced 2026
        if analysis['artifacts']['unnatural_smoothness']:
            factors.append({
                'factor': f"Unnatural smoothness (Laplacian variance: {analysis['artifacts']['smoothness_score']:.2f})",
                'impact': 0.22,  # Increased
                'direction': 'ai',
                'severity': 'high'
            })
            ai_score += 0.22
        
        if analysis['artifacts']['has_grid_pattern']:
            factors.append({
                'factor': 'Grid pattern artifacts (diffusion model signature)',
                'impact': 0.28,  # Increased
                'direction': 'ai',
                'severity': 'high'
            })
            ai_score += 0.28
        
        # NEW 2026: Enhanced Pixel repetition detection
        if analysis['artifacts'].get('has_pixel_repetition', False):
            rep_score = analysis['artifacts'].get('pixel_repetition', 0)
            sim_regions = analysis['artifacts'].get('high_similarity_regions', 0)
            factors.append({
                'factor': f"High pixel repetition detected (repetition: {rep_score:.2f}, similar regions: {sim_regions:.1f})",
                'impact': 0.25,  # Increased
                'direction': 'ai',
                'severity': 'high'
            })
            ai_score += 0.25
        elif analysis['artifacts'].get('pixel_repetition', 0) > 0.3:
            factors.append({
                'factor': f"Moderate pixel repetition detected (score: {analysis['artifacts']['pixel_repetition']:.2f})",
                'impact': 0.15,
                'direction': 'ai',
                'severity': 'medium'
            })
            ai_score += 0.15
        
        # NEW 2026: Edge consistency analysis
        if analysis['artifacts'].get('inconsistent_edges', False):
            factors.append({
                'factor': f"Unnatural edge direction distribution (uniformity: {analysis['artifacts'].get('edge_direction_uniformity', 0):.1f})",
                'impact': 0.15,
                'direction': 'ai',
                'severity': 'medium'
            })
            ai_score += 0.15
        
        # NEW 2026: Contrast analysis
        if analysis['artifacts'].get('unnatural_contrast', False):
            factors.append({
                'factor': f"Unnatural local contrast (value: {analysis['artifacts'].get('local_contrast', 0):.1f})",
                'impact': 0.12,
                'direction': 'ai',
                'severity': 'medium'
            })
            ai_score += 0.12
        
        # NEW 2026: Histogram analysis
        if analysis['artifacts'].get('unnatural_histogram', False):
            factors.append({
                'factor': f"Unnatural histogram distribution (smoothness: {analysis['artifacts'].get('histogram_smoothness', 0):.2f})",
                'impact': 0.10,
                'direction': 'ai',
                'severity': 'low'
            })
            ai_score += 0.10
        
        # NEW 2026: Block pattern analysis
        if analysis['artifacts'].get('unnatural_block_pattern', False):
            factors.append({
                'factor': f"Unnatural block variance pattern (std: {analysis['artifacts'].get('block_variance_std', 0):.1f})",
                'impact': 0.15,
                'direction': 'ai',
                'severity': 'medium'
            })
            ai_score += 0.15
        
        # Edge density - natural photos have complex edges - Enhanced
        if analysis['compression']['edge_density'] > 0.15:
            factors.append({
                'factor': f"Complex edge patterns (density: {analysis['compression']['edge_density']:.3f})",
                'impact': 0.18,  # Increased
                'direction': 'real',
                'severity': 'high'
            })
            real_score += 0.18
        elif analysis['compression']['edge_density'] > 0.12:
            factors.append({
                'factor': f"Good edge patterns (density: {analysis['compression']['edge_density']:.3f})",
                'impact': 0.12,
                'direction': 'real',
                'severity': 'medium'
            })
            real_score += 0.12
        elif analysis['compression']['edge_density'] < 0.03:
            factors.append({
                'factor': f"Very low edge density (density: {analysis['compression']['edge_density']:.3f}) - suspicious",
                'impact': 0.15,
                'direction': 'ai',
                'severity': 'medium'
            })
            ai_score += 0.15
        
        # NEW 2026: Gradient entropy analysis
        if analysis['compression'].get('gradient_entropy', 0) > 3.5:
            factors.append({
                'factor': f"Natural gradient distribution (entropy: {analysis['compression']['gradient_entropy']:.2f})",
                'impact': 0.12,
                'direction': 'real',
                'severity': 'medium'
            })
            real_score += 0.12
        
        # NEW 2026: Combined score validation
        # If multiple strong indicators, increase confidence
        strong_ai_indicators = sum(1 for f in factors if f['direction'] == 'ai' and f['impact'] >= 0.18)
        strong_real_indicators = sum(1 for f in factors if f['direction'] == 'real' and f['impact'] >= 0.18)
        
        if strong_ai_indicators >= 3:
            factors.append({
                'factor': f'Multiple strong AI indicators detected ({strong_ai_indicators} factors)',
                'impact': 0.08,
                'direction': 'ai',
                'severity': 'high'
            })
            ai_score += 0.08
        
        if strong_real_indicators >= 3:
            factors.append({
                'factor': f'Multiple strong real photo indicators ({strong_real_indicators} factors)',
                'impact': 0.08,
                'direction': 'real',
                'severity': 'high'
            })
            real_score += 0.08
        
        return factors, ai_score, real_score

    def _get_enhanced_reasoning(self, prediction, confidence, analysis, factors, ai_score, real_score):
        """Generate comprehensive reasoning"""
        reasons = {
            'summary': [],
            'technical': [],
            'confidence_breakdown': {},
            'risk_assessment': ''
        }
        
        # Confidence assessment
        if confidence < 0.60:
            reasons['summary'].append("‚ö†Ô∏è Low confidence - Image has mixed or ambiguous characteristics")
            reasons['risk_assessment'] = 'inconclusive'
        elif confidence < 0.75:
            reasons['summary'].append("‚ö†Ô∏è Moderate confidence - Some conflicting indicators detected")
            reasons['risk_assessment'] = 'uncertain'
        elif confidence < 0.88:
            reasons['summary'].append("‚úì Good confidence level - Multiple consistent indicators")
            reasons['risk_assessment'] = 'likely'
        else:
            reasons['summary'].append("‚úì High confidence - Strong evidence from multiple sources")
            reasons['risk_assessment'] = 'confident'
        
        # Add top factors (prioritize logo detection)
        sorted_factors = sorted(factors, key=lambda x: x['impact'], reverse=True)
        for factor in sorted_factors[:8]:  # Show more factors
            if factor['severity'] == 'critical':
                icon = "üö®"
            elif factor['severity'] == 'high':
                icon = "üî¥"
            elif factor['severity'] == 'medium':
                icon = "üü°"
            else:
                icon = "üîµ"
            reasons['summary'].append(f"{icon} {factor['factor']}")
        
        # Technical details - Enhanced 2026
        logo_info = ""
        if analysis.get('logo_detection', {}).get('ai_service_detected'):
            logo_info = f" | üö® WATERMARK: {analysis['logo_detection']['ai_service_detected'].upper()}"
        
        reasons['technical'] = [
            f"Noise Level: {analysis['noise']['noise_level']:.2f} ({'Natural range' if analysis['noise']['has_natural_noise'] else 'Suspicious'})",
            f"Noise Gaussian Test: {'Pass' if analysis['noise'].get('is_gaussian_noise', False) else 'Fail'} (œá¬≤={analysis['noise'].get('gaussian_chi_square', 0):.3f})",
            f"Noise Autocorrelation: {analysis['noise'].get('noise_autocorrelation', 0):.3f}",
            f"Noise Spatial Variance: {analysis['noise'].get('noise_spatial_variance', 0):.2f}",
            f"Noise Spectrum Entropy: {analysis['noise'].get('noise_spectrum_entropy', 0):.2f}",
            f"Texture Consistency: {analysis['texture']['texture_consistency']:.2f}",
            f"Texture Anomaly Score: {analysis['texture'].get('texture_anomaly_score', 0)}/3",
            f"Gradient Uniformity: {analysis['texture'].get('gradient_uniformity', 0):.2f}",
            f"LBP Uniformity: {analysis['texture'].get('lbp_uniformity', 0):.2f}",
            f"Color Entropy: {analysis['color']['entropy']:.2f} bits",
            f"Frequency Balance: {analysis['frequency']['freq_balance']:.3f}",
            f"Phase Coherence: {analysis['frequency'].get('phase_coherence', 0):.3f}",
            f"Radial Variance: {analysis['frequency'].get('radial_variance', 0):.0f}",
            f"Color Diversity: {analysis['color']['color_diversity']:.3f}",
            f"Smoothness Score: {analysis['artifacts']['smoothness_score']:.2f}",
            f"Pixel Repetition: {analysis['artifacts'].get('pixel_repetition', 0):.3f}",
            f"Edge Direction Uniformity: {analysis['artifacts'].get('edge_direction_uniformity', 0):.1f}",
            f"Local Contrast: {analysis['artifacts'].get('local_contrast', 0):.1f}",
            f"Histogram Smoothness: {analysis['artifacts'].get('histogram_smoothness', 0):.2f}",
            f"Block Variance Std: {analysis['artifacts'].get('block_variance_std', 0):.1f}",
            f"Color Cluster Variance (5): {analysis['color'].get('cluster_variance_5', 0):.0f}",
            f"Color Cluster Variance (10): {analysis['color'].get('cluster_variance_10', 0):.0f}",
            f"Saturation Entropy: {analysis['color'].get('saturation_entropy', 0):.2f}",
            f"Color Gradient Variance: {analysis['color'].get('color_gradient_variance', 0):.1f}",
            f"Histogram Smoothness: {analysis['color'].get('histogram_smoothness', 0):.2f}",
            f"Gradient Entropy: {analysis['compression'].get('gradient_entropy', 0):.2f}",
            f"Block Boundary Score: {analysis['compression'].get('block_boundary_score', 0):.3f}",
            f"DCT Energy: {analysis['compression'].get('dct_energy', 0):.0f}",
            f"EXIF Data: {'Present' if analysis['metadata']['has_exif'] else 'Missing'}{logo_info}",
            f"Compression: {'Natural JPEG' if analysis['compression']['natural_compression'] else 'Unusual/None'}",
            f"Edge Density: {analysis['compression']['edge_density']:.3f}",
            f"Noise Energy: {analysis['noise']['noise_energy']:.0f}",
            f"Watermark Score: {analysis.get('logo_detection', {}).get('watermark_score', 0):.2f}"
        ]
        
        # NEW v4.0: Add forensic analysis metrics
        forensics = analysis.get('forensics', {})
        if forensics:
            reasons['technical'].extend([
                f"ELA Score: {forensics.get('ela', {}).get('ela_score', 0):.2f} (variance: {forensics.get('ela', {}).get('ela_variance', 0):.1f})",
                f"Double JPEG: {'Detected' if forensics.get('double_jpeg', {}).get('has_double_compression', False) else 'Not detected'} (score: {forensics.get('double_jpeg', {}).get('double_jpeg_score', 0):.1f})",
                f"Copy-Move: {'Detected' if forensics.get('copy_move', {}).get('has_copy_move', False) else 'Not detected'} (score: {forensics.get('copy_move', {}).get('copy_move_score', 0):.3f})",
                f"Splicing: {'Detected' if forensics.get('splicing', {}).get('has_splicing', False) else 'Not detected'} (score: {forensics.get('splicing', {}).get('splicing_score', 0):.3f})",
                f"JPEG Quality Estimate: {forensics.get('jpeg_quality', {}).get('estimated_jpeg_quality', 100)}"
            ])
        
        # NEW v4.0: Add AI signature metrics
        ai_sigs = analysis.get('ai_signatures', {})
        if ai_sigs:
            reasons['technical'].extend([
                f"GAN Fingerprint: {'Detected' if ai_sigs.get('gan_fingerprint', {}).get('gan_fingerprint_detected', False) else 'Not detected'} (score: {ai_sigs.get('gan_fingerprint', {}).get('gan_score', 0):.3f})",
                f"Diffusion Signature: {'Detected' if ai_sigs.get('diffusion_signature', {}).get('diffusion_signature_detected', False) else 'Not detected'} (score: {ai_sigs.get('diffusion_signature', {}).get('diffusion_score', 0):.3f})",
                f"Adversarial Perturbation: {'Detected' if ai_sigs.get('adversarial_perturbation', {}).get('has_adversarial_noise', False) else 'Not detected'} (score: {ai_sigs.get('adversarial_perturbation', {}).get('perturbation_score', 0):.3f})"
            ])
        
        # NEW v4.0: Add advanced texture metrics
        adv_texture = analysis.get('advanced_texture', {})
        if adv_texture.get('lbp'):
            reasons['technical'].append(
                f"LBP Analysis: Entropy={adv_texture['lbp'].get('lbp_entropy', 0):.2f}, Uniformity={adv_texture['lbp'].get('lbp_uniformity', 0):.4f}"
            )
        
        # NEW v4.0: Add biometric metrics
        biometric = analysis.get('biometric', {})
        if biometric.get('face_manipulation', {}).get('faces_detected', 0) > 0:
            reasons['technical'].append(
                f"Face Analysis: {biometric['face_manipulation'].get('faces_detected', 0)} face(s), Manipulation: {'Detected' if biometric['face_manipulation'].get('has_face_manipulation', False) else 'Not detected'}"
            )
        
        # NEW v4.0: Add statistical metrics
        statistical = analysis.get('statistical', {})
        if statistical.get('benford_law'):
            reasons['technical'].append(
                f"Benford's Law: Score={statistical['benford_law'].get('benford_score', 0):.3f}, Follows: {statistical['benford_law'].get('follows_benford', False)}"
            )
        
        # Confidence breakdown
        reasons['confidence_breakdown'] = {
            'ai_indicators_score': round(ai_score, 3),
            'real_indicators_score': round(real_score, 3),
            'model_confidence': round(confidence, 4),
            'total_factors': len(factors),
            'score_difference': round(abs(ai_score - real_score), 3)
        }
        
        return reasons

    def classify_image(self, image, file_size=0):
        try:
            # Validate input
            if image is None:
                raise ValueError("Image cannot be None")
            
            # Comprehensive analysis
            analysis = self._comprehensive_analysis(image)
            analysis['file_size_kb'] = max(0, file_size / 1024) if file_size > 0 else 0
            
            # Model prediction
            if self.model is None:
                raise RuntimeError("Model not properly initialized")
            
            input_tensor = self.preprocess(image).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                output = self.model(input_tensor)
                probabilities = torch.nn.functional.softmax(output[0], dim=0)
                confidence, predicted = torch.max(probabilities, 0)
            
            confidence_value = float(confidence.item())
            prediction = self.classes[predicted]
            
            # Calculate enhanced confidence factors
            factors, ai_score, real_score = self._calculate_confidence_factors(analysis)
            
            # IMPROVED confidence adjustment logic - 2026 Enhanced Ensemble
            adjusted_confidence = confidence_value
            score_diff = abs(ai_score - real_score)
            
            # NEW 2026: Advanced Weighted ensemble voting with dynamic weights
            # Adjust weights based on score difference (more reliable method gets more weight)
            if score_diff > 0.40:
                # Strong heuristic signal - give heuristics more weight
                model_weight = 0.25
                heuristic_weight = 0.75
            elif score_diff > 0.25:
                model_weight = 0.35
                heuristic_weight = 0.65
            else:
                # Balanced - use standard weights
                model_weight = 0.4
                heuristic_weight = 0.6
            
            # Calculate ensemble prediction with confidence scaling
            if ai_score > real_score:
                heuristic_prediction = 'AI-generated'
                # Scale confidence based on score difference
                heuristic_confidence = min(0.98, 0.50 + min(0.48, (ai_score - real_score) * 1.2))
            else:
                heuristic_prediction = 'Real'
                heuristic_confidence = min(0.98, 0.50 + min(0.48, (real_score - ai_score) * 1.2))
            
            # Ensemble confidence with better fusion
            if prediction == heuristic_prediction:
                # Agreement: boost confidence significantly
                ensemble_confidence = (model_weight * confidence_value) + (heuristic_weight * heuristic_confidence)
                if score_diff > 0.40:
                    adjusted_confidence = min(0.99, ensemble_confidence + 0.08)  # Strong agreement
                elif score_diff > 0.30:
                    adjusted_confidence = min(0.97, ensemble_confidence + 0.06)
                elif score_diff > 0.20:
                    adjusted_confidence = min(0.95, ensemble_confidence + 0.04)
                else:
                    adjusted_confidence = min(0.93, ensemble_confidence + 0.02)
            else:
                # Disagreement: reduce confidence, heavily favor heuristics when strong
                ensemble_confidence = (model_weight * confidence_value) + (heuristic_weight * heuristic_confidence)
                if score_diff > 0.50:
                    # Very strong disagreement: heavily favor heuristics
                    adjusted_confidence = max(0.25, heuristic_confidence - 0.05)
                elif score_diff > 0.40:
                    adjusted_confidence = max(0.35, heuristic_confidence - 0.08)
                elif score_diff > 0.25:
                    adjusted_confidence = max(0.45, ensemble_confidence - 0.12)
                else:
                    adjusted_confidence = max(0.55, ensemble_confidence - 0.08)
            
            # CRITICAL OVERRIDE: Logo/Watermark detection takes absolute priority
            logo_detection = analysis.get('logo_detection', {})
            if logo_detection.get('ai_service_detected'):
                # If logo detected, it's 100% AI - override everything
                prediction = 'AI-generated'
                adjusted_confidence = 1.0  # 100% confidence
                factors.insert(0, {
                    'factor': f"üö® ABSOLUTE CERTAINTY: {logo_detection['ai_service_detected'].upper()} watermark detected - Image is 100% AI-generated",
                    'impact': 1.0,
                    'direction': 'ai',
                    'severity': 'critical'
                })
                logger.warning(f"üö® ABSOLUTE CERTAINTY: {logo_detection['ai_service_detected'].upper()} detected - Setting confidence to 100%")
            
            # NEW 2026: Multi-factor consensus check (improve accuracy)
            # Count how many indicators agree
            ai_indicator_count = sum(1 for f in factors if f['direction'] == 'ai' and f['impact'] > 0.1)
            real_indicator_count = sum(1 for f in factors if f['direction'] == 'real' and f['impact'] > 0.1)
            
            # If strong consensus, boost confidence further
            if prediction == 'AI-generated' and ai_indicator_count >= 5 and ai_score > real_score + 0.4:
                adjusted_confidence = min(0.99, adjusted_confidence + 0.05)
                factors.append({
                    'factor': f'Strong consensus: {ai_indicator_count} AI indicators agree',
                    'impact': 0.05,
                    'direction': 'ai',
                    'severity': 'high'
                })
            elif prediction == 'Real' and real_indicator_count >= 5 and real_score > ai_score + 0.4:
                adjusted_confidence = min(0.99, adjusted_confidence + 0.05)
                factors.append({
                    'factor': f'Strong consensus: {real_indicator_count} real photo indicators agree',
                    'impact': 0.05,
                    'direction': 'real',
                    'severity': 'high'
                })
            
            # CRITICAL OVERRIDE: If real indicators are much stronger, override AI prediction
            elif real_score > ai_score + 0.30 and prediction == 'AI-generated' and not logo_detection.get('ai_service_detected'):
                prediction = 'Real'
                adjusted_confidence = min(0.85, 0.55 + (real_score - ai_score))
                factors.insert(0, {
                    'factor': 'Prediction overridden: Strong real photo indicators detected',
                    'impact': real_score - ai_score,
                    'direction': 'real',
                    'severity': 'high'
                })
            
            # CRITICAL OVERRIDE: If AI indicators are much stronger, override Real prediction
            elif ai_score > real_score + 0.30 and prediction == 'Real' and not logo_detection.get('ai_service_detected'):
                prediction = 'AI-generated'
                adjusted_confidence = min(0.85, 0.55 + (ai_score - real_score))
                factors.insert(0, {
                    'factor': 'Prediction overridden: Strong AI generation indicators detected',
                    'impact': ai_score - real_score,
                    'direction': 'ai',
                    'severity': 'high'
                })
            
            # NEW 2026: Final accuracy validation and calibration
            # Cross-validate prediction with multiple independent checks
            validation_score = 0
            validation_checks = []
            
            # Check 1: Logo detection (highest priority)
            if logo_detection.get('ai_service_detected'):
                validation_score += 1.0
                validation_checks.append('logo_detected')
            
            # Check 2: Noise pattern (very reliable)
            if analysis['noise']['has_natural_noise']:
                validation_score += 0.3
                validation_checks.append('natural_noise')
            elif analysis['noise']['suspiciously_clean']:
                validation_score -= 0.3
                validation_checks.append('clean_noise')
            
            # Check 3: Compression artifacts
            if analysis['compression']['natural_compression']:
                validation_score += 0.25
                validation_checks.append('natural_compression')
            elif analysis['compression'].get('ai_compression_pattern', False):
                validation_score -= 0.25
                validation_checks.append('ai_compression')
            
            # Check 4: Texture analysis
            if analysis['texture'].get('texture_anomaly_score', 0) >= 2:
                validation_score -= 0.2
                validation_checks.append('texture_anomaly')
            
            # Check 5: Color analysis
            if analysis['color']['entropy'] > 7.0:
                validation_score += 0.15
                validation_checks.append('high_entropy')
            elif analysis['color']['entropy'] < 4.5:
                validation_score -= 0.15
                validation_checks.append('low_entropy')
            
            # Apply validation adjustment
            if validation_score > 0.3 and prediction == 'Real':
                adjusted_confidence = min(0.99, adjusted_confidence + 0.03)
            elif validation_score < -0.3 and prediction == 'AI-generated':
                adjusted_confidence = min(0.99, adjusted_confidence + 0.03)
            elif validation_score > 0.3 and prediction == 'AI-generated':
                adjusted_confidence = max(0.40, adjusted_confidence - 0.10)
            elif validation_score < -0.3 and prediction == 'Real':
                adjusted_confidence = max(0.40, adjusted_confidence - 0.10)
            
            # Get enhanced reasoning
            reasoning = self._get_enhanced_reasoning(
                prediction, adjusted_confidence, analysis, factors, ai_score, real_score
            )
            
            # Add validation info to reasoning
            if validation_checks:
                reasoning['validation'] = {
                    'validation_score': round(validation_score, 3),
                    'checks_passed': validation_checks,
                    'confidence_adjusted': abs(validation_score) > 0.3
                }
            
            # Create analysis summary for frontend compatibility
            analysis_summary = {
                'noise_level': analysis['noise']['noise_level'],
                'texture_score': analysis['texture']['texture_consistency'],
                'color_entropy': analysis['color']['entropy'],
                'has_exif': analysis['metadata']['has_exif'],
                'dimensions': analysis['dimensions'],
                'color_diversity': analysis['color']['color_diversity'],
                'smoothness': analysis['artifacts']['smoothness_score'],
                'frequency_balance': analysis['frequency']['freq_balance'],
                'compression_natural': analysis['compression']['natural_compression'],
                'edge_density': analysis['compression']['edge_density']
            }
            
            return {
                'prediction': prediction,
                'confidence': round(adjusted_confidence, 4),
                'raw_confidence': round(confidence_value, 4),
                'probabilities': {
                    'AI-generated': round(float(probabilities[0]), 4),
                    'Real': round(float(probabilities[1]), 4)
                },
                'analysis': analysis,
                'analysis_summary': analysis_summary,  # Added for frontend compatibility
                'reasoning': reasoning,
                'confidence_factors': factors,
                'heuristic_scores': {
                    'ai_score': round(ai_score, 3),
                    'real_score': round(real_score, 3),
                    'agreement': 'strong' if score_diff < 0.10 else ('moderate' if score_diff < 0.20 else 'weak')
                }
            }
            
        except Exception as e:
            logger.error(f"Error in classify_image: {str(e)}")
            raise


@trueshot_ai.route('/')
def index():
    return render_template('trueshot.html')

@trueshot_ai.route('/analyze', methods=['POST'])
@rate_limit_strict(requests_per_minute=5, requests_per_hour=50)  # Strict limit for image processing
def analyze():
    """
    Analyze image authenticity
    OWASP: Rate limited, file validation
    """
    try:
        if 'image' not in request.files:
            return jsonify({
                'status': 'error',
                'message': 'No image file provided'
            }), 400
        
        file = request.files['image']
        
        if file.filename == '':
            return jsonify({
                'status': 'error',
                'message': 'No file selected'
            }), 400
        
        # Validate filename
        try:
            filename = InputValidator.validate_filename(file.filename, 'filename', required=True)
        except Exception as e:
            return jsonify({
                'status': 'error',
                'message': f'Invalid filename: {str(e)}'
            }), 400
        
        allowed_extensions = ('.png', '.jpg', '.jpeg', '.webp', '.bmp', '.tiff')
        if not filename.lower().endswith(allowed_extensions):
            return jsonify({
                'status': 'error',
                'message': 'Invalid file type. Supported: PNG, JPG, JPEG, WebP, BMP, TIFF'
            }), 400
        
        # Read image
        image_bytes = file.read()
        if not image_bytes:
            return jsonify({
                'status': 'error',
                'message': 'Empty file uploaded'
            }), 400
        
        file_size = len(image_bytes)
        
        # Validate file size (50MB limit)
        if file_size > 50 * 1024 * 1024:
            return jsonify({
                'status': 'error',
                'message': 'File size exceeds 50MB limit'
            }), 400
        
        # Minimum file size check
        if file_size < 100:
            return jsonify({
                'status': 'error',
                'message': 'File too small to be a valid image'
            }), 400
        
        # Open and process image with better error handling
        try:
            image = Image.open(io.BytesIO(image_bytes))
            image.verify()  # Verify it's a valid image
        except Exception as e:
            return jsonify({
                'status': 'error',
                'message': f'Invalid image file: {str(e)}'
            }), 400
        
        # Reopen after verify (verify closes the file)
        image = Image.open(io.BytesIO(image_bytes))
        
        # Validate image dimensions
        width, height = image.size
        if width < 32 or height < 32:
            return jsonify({
                'status': 'error',
                'message': 'Image dimensions too small (minimum 32x32 pixels)'
            }), 400
        
        if width > 10000 or height > 10000:
            return jsonify({
                'status': 'error',
                'message': 'Image dimensions too large (maximum 10000x10000 pixels)'
            }), 400
        
        # Convert to RGB
        try:
            if image.mode in ('RGBA', 'P', 'L'):
                image = image.convert('RGB')
        except Exception as e:
            return jsonify({
                'status': 'error',
                'message': f'Failed to convert image to RGB: {str(e)}'
            }), 400
        
        # Vision analysis (caption, colors, objects) - additional context
        vision_data = None
        if VISION_ANALYZER_AVAILABLE:
            try:
                vision_data = analyze_image(image, n_colors=5)
                logger.info(f"‚úì Vision analysis complete (source: {vision_data.get('analysis_source', 'unknown')})")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Vision analysis failed: {e}")
        
        # Analyze image authenticity with timeout protection
        try:
            analyzer = UltraAdvancedImageAnalyzer()
            result = analyzer.classify_image(image, file_size)
        except torch.cuda.OutOfMemoryError:
            return jsonify({
                'status': 'error',
                'message': 'GPU out of memory. Please try a smaller image or use CPU mode.'
            }), 500
        except Exception as e:
            return jsonify({
                'status': 'error',
                'message': f'Analysis failed: {str(e)}'
            }), 500
        
        # Add metadata
        result['timestamp'] = datetime.now().isoformat()
        result['image_hash'] = hashlib.md5(image_bytes).hexdigest()[:16]
        result['filename'] = file.filename
        
        # Add vision analysis data if available
        if vision_data:
            result['vision_analysis'] = {
                'caption': vision_data.get('caption', ''),
                'dominant_colors': vision_data.get('dominant_colors', []),
                'objects_detected': vision_data.get('objects_detected', []),
                'analysis_source': vision_data.get('analysis_source', 'local')
            }
        
        # Convert all values to JSON-serializable types
        def convert_to_serializable(obj):
            if isinstance(obj, np.bool_):
                return bool(obj)
            elif isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {key: convert_to_serializable(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_to_serializable(item) for item in obj]
            return obj
        
        result = convert_to_serializable(result)
        
        return jsonify({
            'status': 'success',
            'result': result,
            'message': 'Analysis completed successfully'
        })
        
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': f'Analysis failed: {str(e)}'
        }), 500

@trueshot_ai.route('/health', methods=['GET'])
def health_check():
    return jsonify({
        'status': 'healthy',
        'model_loaded': True,
        'device': 'cuda' if torch.cuda.is_available() else 'cpu',
        'timestamp': datetime.now().isoformat(),
        'version': '4.0.0',
        'features': {
            'logo_detection': True,
            'watermark_detection': OCR_AVAILABLE,
            'enhanced_frequency_analysis': True,
            'forensic_analysis': SCIPY_AVAILABLE,
            'ai_signature_detection': True,
            'face_manipulation_detection': True,
            'perceptual_hashing': IMAGEHASH_AVAILABLE,
            'advanced_texture_analysis': SKIMAGE_AVAILABLE,
            'statistical_analysis': SCIPY_AVAILABLE,
            'ultra_advanced_v4': True
        }
    })

@trueshot_ai.route('/batch-analyze', methods=['POST'])
def batch_analyze():
    """Analyze multiple images at once with improved error handling"""
    try:
        files = request.files.getlist('images')
        if not files or len(files) == 0:
            return jsonify({
                'status': 'error',
                'message': 'No images provided'
            }), 400
        
        if len(files) > 10:
            return jsonify({
                'status': 'error',
                'message': 'Maximum 10 images per batch'
            }), 400
        
        analyzer = UltraAdvancedImageAnalyzer()
        results = []
        
        # Helper function for serialization
        def convert_to_serializable(obj):
            if isinstance(obj, np.bool_):
                return bool(obj)
            elif isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {key: convert_to_serializable(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_to_serializable(item) for item in obj]
            return obj
        
        for file in files:
            try:
                # Validate file
                if not file.filename:
                    results.append({
                        'status': 'error',
                        'filename': 'unknown',
                        'message': 'No filename provided'
                    })
                    continue
                
                image_bytes = file.read()
                if not image_bytes or len(image_bytes) < 100:
                    results.append({
                        'status': 'error',
                        'filename': file.filename,
                        'message': 'File too small or empty'
                    })
                    continue
                
                if len(image_bytes) > 50 * 1024 * 1024:
                    results.append({
                        'status': 'error',
                        'filename': file.filename,
                        'message': 'File size exceeds 50MB limit'
                    })
                    continue
                
                # Open and validate image
                try:
                    image = Image.open(io.BytesIO(image_bytes))
                    image.verify()
                    image = Image.open(io.BytesIO(image_bytes))
                except Exception as e:
                    results.append({
                        'status': 'error',
                        'filename': file.filename,
                        'message': f'Invalid image: {str(e)}'
                    })
                    continue
                
                # Validate dimensions
                width, height = image.size
                if width < 32 or height < 32 or width > 10000 or height > 10000:
                    results.append({
                        'status': 'error',
                        'filename': file.filename,
                        'message': 'Image dimensions out of valid range (32-10000 pixels)'
                    })
                    continue
                
                # Convert to RGB
                if image.mode in ('RGBA', 'P', 'L'):
                    image = image.convert('RGB')
                
                # Analyze
                result = analyzer.classify_image(image, len(image_bytes))
                result['filename'] = file.filename
                result = convert_to_serializable(result)
                
                results.append({
                    'status': 'success',
                    'result': result
                })
            except torch.cuda.OutOfMemoryError:
                results.append({
                    'status': 'error',
                    'filename': file.filename,
                    'message': 'GPU out of memory'
                })
            except Exception as e:
                results.append({
                    'status': 'error',
                    'filename': file.filename,
                    'message': str(e)
                })
        
        return jsonify({
            'status': 'success',
            'results': results,
            'total': len(files),
            'successful': sum(1 for r in results if r['status'] == 'success')
        })
        
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

# Blueprint is registered in server.py
# This module should only define the blueprint